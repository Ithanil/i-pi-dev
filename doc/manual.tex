%% LyX 2.0.0 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[12pt,english,fleqn]{report}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
\usepackage{babel}
\usepackage{verbatim}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 1},backref=section,colorlinks=false]
 {hyperref}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\@ifundefined{definecolor} {\usepackage{color}}{}
\usepackage{multicol}\makeindex
\usepackage{xspace}
\usepackage[numbers]{natbib}

\makeatother

\begin{document}
\newcommand{\dd}{\; \mathrm{d}} 
\newcommand{\Tr}{\mathrm{Tr}} 
\newcommand{\bra}{< \! \!} 
\newcommand{\ket}{\! \! >} 
\newcommand{\betan}{\beta_N} 
\newcommand{\logn}{\mathrm{ln}} 
\newcommand{\expon}{\mathrm{exp}} 
\newcommand{\Imag}{\mathrm{Im}}
\newcommand{\ipi}{{\sc i-PI}\xspace}
\newcommand{\DFT}{Quantum Espresso, CP2K, CPMD and FHI-AIMS }
\newcommand{\empirical}{LAMMPS}

\begin{titlepage} 

\begin{center}
\vspace*{5.5cm}

\par\end{center}

\begin{center}
{\LARGE \ipi}
\par\end{center}{\LARGE \par}

\begin{center}
A Python wrapper for Path Integral Molecular Dynamics
\par\end{center}

\end{titlepage}

\pagenumbering{roman}

\tableofcontents{}\listoffigures


\newpage{}\pagenumbering{arabic}


\chapter{About \ipi}

\label{intro}

\ipi is a Path Integral Molecular Dynamics (PIMD) wrapper code written
in Python, designed to be used together with \emph{ab initio} evaluation
of the interactions between the atoms. The main goal is to decouple
the problem of evolving the ionic positions to sample the appropriate
thermodynamic ensemble and the problem of computing the inter-atomic
forces. 

The implementation is based on a client-server paradigm, where \ipi
acts as the server and deals with the propagation of the nuclear dynamics,
whereas the calculation of the potential energy, forces and the potential
energy part of the pressure virial is delegated to one or more instances
of an external code, acting as clients. Since the main focus is on
performing \emph{ab initio} PIMD -- where the cost of the force evaluation
is overwhelming relative to the ionic dynamics -- clarity has been
privileged over speed. Still, the implementation of \ipi is efficient
enough that it can be used with empirical forcefields to perform simple
benchmarks and preparatory simulations. 


\section{Manual structure}

This manual will be structured as follows: 
\begin{itemize}
\item In chapter \ref{intro} we briefly discuss the basis for PIMD and
some of the specialized techniques used in \ipi. 
\item In chapter \ref{getstarted} we will discuss how to install the code
and test that it is working, and give a brief tutorial on running
simulations with \ipi.
\item In chapter \ref{user} we explain in more detail the form of the input
and output files and how the communication between the client and
server codes is done.
\item In chapter \ref{hierarchy} a full list of the major classes used
in the code is given, along with the appropriate tag names and a brief
description of all the fields that can be specified in the xml input
file.
\item In chapter \ref{trouble} we list some of the more commonly encountered
problems, and their solutions.
\end{itemize}

\section{Path Integral Molecular Dynamics}

Molecular dynamics (MD) is a technique used to study the properties
of a system of interacting particles by applying Newton's equations
of motion to produce trajectories which can be used to efficiently
explore the phase space. This can be used to calculate many equilibrium
and dynamical properties, study systems from isolated gas molecules
to condensed phase bulk materials, and can be applied to systems from
the atomistic scale to mesoscopic colloidal particles.

However, while this technique has been very successful, in most MD
implementations the assumption is made that the nuclei behave as classical
particles, which for light nuclei such as hydrogen is often a very
poor approximation as the effect of zero-point energy (ZPE) and quantum
tunnelling can be large. For example, even at room temperature the
vibrational frequency of an OH stretch in water is over 15 times larger
than the available thermal energy, and so this motion will be highly
quantized. The current state-of-the-art method to include nuclear
quantum effects (NQE) in the calculation of static properties of condensed
phase systems is path integral molecular dynamics (PIMD).

PIMD generates the quantum-mechanical ensemble of a system of interacting
particles by using MD in an extended phase space. This is derived
from the path integral formalism of \cite{feyn-hibb65book}, which
relates the statistics of a collection of quantum particles to those
of a set of classical ring polymers, a ring polymer being a number
of replicas of a particle coupled by harmonic springs. This so-called
classical isomorphism is exact in the limit as the number of replicas
goes to infinity, but in practice is converged numerically with only
a finite number.

This then allows quantum phase space averages to be calculated from
classical trajectories, with only about an order of magnitude more
computing time than would be required for standard MD. Also, since
PIMD is simply classical MD in an extended phase space many of the
techniques developed to improve the scope and efficiency of MD simulations
can be applied straightforwardly to the equivalent PIMD calculations
\cite{ceri+10jcp,mart+99jcp}. Finally, several techniques designed
specifically for PIMD simulations are now available to increase the
rate of convergence with respect to the number of replicas used \cite{mark-mano08jcp,ceri+11jcp},
so further reducing the computational overhead of the method. All
of these facts mean that it is now feasible to do PIMD simulations
with thousands of molecules, or even using \emph{ab initio} electronic
structure calculations to propagate the dynamics for small systems.

Furthermore, the framework used to run PIMD simulations can be adapted
to generate approximate quantum dynamical information \cite{crai-mano04jcp,hone+06jcp},
and so can also be used to calculate correlation functions. While
the dynamics is classical, and so real-time quantum coherences cannot
be captured, the correct capture of quantum statistical information
and the rapid decoherence observed in condensed phase systems mean
that in many cases very accurate results can be obtained from such
approximate treatments of quantum dynamics \cite{habe+13arpc}.


\section{Implementation}


\subsection{Automated evaluation (depend objects)}

\ipi uses a caching mechanism with automatic value updating to make
the code used to propagate the dynamics as simple and clear as possible.
Every physical quantity that is referenced in the code is created
using a {}``depend'' object class, which is given the parameters
on which it depends and a function used to calculate its value. 

{}``Depend'' objects can be called to get the physical quantity
they represent. However, they have further functionality. Firstly,
once the value of a {}``depend'' object has been calculated, its
value is cached, so further references to that quantity will not need
to evaluate the function that calculates it. Furthermore, the code
keeps track of when any of the dependencies of the variable are updated,
and makes sure that the quantity is automatically recomputed when
it is needed. 

This choice makes implementation slightly more complex when the physical
observables are first introduced as variables, as one has to take
care of stating their dependencies as well as the function that computes
them. However, the advantage is that when the physical quantities
are used, in the integrator of the dynamics or in the evaluation of
physical properties, one does not need to take care of book-keeping
and the code can be cleaner, transparent and readable.


\subsection{Communication protocol}

Since \ipi is designed to be used with a wide range of codes and
platforms, it has to rely on a simple and robust method for communicating
between the server and client. Even though other choices are possible,
and it should be relatively simple to implement other means of communication,
the preferred approach relies on sockets as the underlying infrastructure.
Both Internet and Unix domain sockets can be used: the latter allow
for fast communication on a single node, whereas the former makes
it possible to realize a distributed computing paradigm, with clients
running on different nodes or even on different HPC facilities. In
order to facilitate implementation of the sockets communication in
client codes, a simple set of C wrappers to the standard libraries
socket implementation is provided as part of the \ipi distribution,
that can be used in any programming language that can be linked with
C code

As far as the communication protocol is concerned, the guiding principle
has been keeping it to the least common denominator, and avoiding
any feature that may be code-specific. Only minimal amount of information
is transferred between the client and the server; the position of
the atoms and cell parameters in one direction, and the forces, virial
and potential in the other.

For more details about sockets and communication, see \ref{distrib}. 


\subsection{Internal units}

\label{units}

All the units used internally by \ipi are atomic units, as given
below. By default, both input and output data are given in atomic
units, but in most cases the default units can be overridden if one
wishes so. For details on how to do this, see \ref{inputunits} and
\ref{propertyfile}.

\begin{center}
\begin{tabular}{lll}
\hline 
Unit & Name & S.I. Value\tabularnewline
\hline 
\hline 
Length & Bohr radius & 5.2917721e-11 m\tabularnewline
Time & N.A. & 2.4188843e-17 s\tabularnewline
Mass & Electron mass & 9.1093819e-31 kg\tabularnewline
Temperature & Hartree & 315774.66 K\tabularnewline
Energy & Hartree & 4.3597438e-18 J\tabularnewline
Pressure & N.A. & 2.9421912e13 Pa\tabularnewline
\hline 
\end{tabular}
\par\end{center}


\section{Core features}

The functionality of \ipi includes:
\begin{itemize}
\item Integrators for constant temperature ensembles, including: \begin{itemize}
\item Local and global stochastic thermostats \cite{plangevin1908cras,buss-parr08cpc}, with optional optimized sampling over the normal mode coordinates \cite{ceri+10jcp}.
\item Optimal sampling generalized Langevin equation (GLE) thermostats \cite{ceri+09jctc}.
\item Path integral + GLE (PI+GLE) thermostats \cite{ceri+11jcp} for accelerating convergence of potential energy with respect to number of replicas, as well as the more recent PIGLET method \cite{ceri-mano12prl} which also converges kinetic energy quickly.
\end{itemize}
\item Integrators for constant pressure ensembles \cite{mart+99jcp,buss+09jpc}.
\item Ring polymer contraction \cite{mark-mano08jcp}.
\item Scaled path finite difference energy and heat capacity estimators
\cite{tyamamoto05jcp}.
\item Displaced path momentum distribution estimator \cite{linlin+10prl}.
\item Dynamical property calculation modes:\begin{itemize}
\item Ring polymer molecular dynamics \cite{crai-mano04jcp}.
\item Partially-adiabatic centroid molecular dynamics \cite{habe+08jcp,hone+06jcp}.
\end{itemize}
\end{itemize}

\section{Licence and credits}

This code is distributed under the GPL licence. For more details see
\url{www.gnu.org/licences/gpl.html}. If you use this code in any
future publications, please cite this using {[}cpc paper citation{]}.


\section{On-line resources}


\subsection{Python resources}

For help with Python programming, see \url{www.python.org}. For information
about the NumPy mathematical library, see \url{www.numpy.org}, and
for worked examples of its capabilities see \url{www.scipy.org/Tentative_NumPy_Tutorial}.
Finally, see \url{http://hgomersall.github.io/pyFFTW/} for documentation
on the Python FFTW library that is currently implemented with the
wrapper.


\subsection{Client code resources}

\label{librarywebsites}

There are currently patches available for Quantum Espresso version
4.3.2 and CP2K version 2.2. It should be possible to adapt these patches
to other versions of the codes with minor modifications. For more
information about Quantum Espresso and CP2K, go to \url{www.quantum-espresso.org}
and \url{cp2k.org} respectively.

There are several Fortran and C libraries that most client codes will
probably need to run, such as FFTW, BLAS and LAPACK. These can be
found at \url{www.fftw.org}, \url{www.netlib.org/blas} and \url{www.netlib.org/lapack}
respectively.

These codes do not come as part of the \ipi package, and must be
downloaded separately. See chapter~\ref{install} for more details
of how to do this. 


\subsection{\ipi resources}

For more information about \ipi{} and to download the source code
go to \url{gle4md.berlios.de }, where one can also obtain colored-noise
parameters to run Path Integral with Generalized Langevin Equation
thermostat (PI+GLE) calculations.


\chapter{Getting started}

\label{getstarted}


\section{Installing \ipi}

\label{install}


\subsection{Requirements}

To install and run \ipi, you will need to have:
\begin{itemize}
\item Python version 2.4 or greater
\item The Python numerical library NumPy
\end{itemize}
Note that \ipi does not need to be compiled, it can be run as a script
as long as the appropriate libraries have been installed.

Additionally, most client codes will have their own requirements.
Many of them, including the test client codes given in the {}``forces''
directory, will need a suitable Fortran compiler. A C compiler is
required for the sockets.c wrapper to the sockets standard library.
Most electronic structure codes will also need to be linked with some
mathematical libraries, such as BLAS, FFTW and LAPACK. Installation
instructions for these codes should be provided as part of the code
distribution and on the appropriate website, as given in \ref{librarywebsites}.
Patching for use with \ipi{} should not introduce further dependencies.


\subsection{\ipi download}

A tar file can be downloaded from the website \url{gle4md.berlios.de}.
To install this you need to input the following command:

\begin{verbatim}
> tar xf [wrapper_tar_file.tar]
\end{verbatim}

You can also obtain a local clone of the git repository on \url{bitbucket.org}
using:

\begin{verbatim}
> git clone [github repository name]
\end{verbatim}


\subsection{Installing NumPy}

NumPy is the standard Python mathematics library, and is used for
most of the array manipulation and linear algebra in \ipi. It should
be installed alongside most standard Python environments on HPC facilities.
Otherwise, it is generally relatively straightforward to install it. 

In any case you must first obtain the NumPy code, which can be downloaded
as a tar file from \url{http://www.numpy.org}. If the version of
NumPy being installed is given by {}``np\_vers'', this can be extracted
using:

\begin{verbatim}
> tar czf np_vers.tar.gz
\end{verbatim}

Before installing this code it first needs to be configured correctly.
Note that this requires the distutils package that comes with the
python-dev package. Assuming that the required software is installed,
the NumPy package is built using:

\begin{verbatim}
> python setup.py build
\end{verbatim}

The next step is to install NumPy. By default the download is to the
directory /usr/local. If you have root access, and so can write to
/usr, then all that needs to be done to finish the install is:

\begin{verbatim}
> python setup.py install
\end{verbatim}

If you do not have root access, then the next step depends on which
version of Python is beind used. With versions 2.6 or later there
is a simple command to automatically download into the directory \$HOME/local:

\begin{verbatim}
> python setup.py install --user
\end{verbatim}

With Python 2.4/2.5 the process is a little more involved. First you
must explicitly install the package in the directory of choice, {}``np\_dir''
say, with the following command:

\begin{verbatim}
> python setup.py install --prefix=np_dir
\end{verbatim}

Next, you must tell python where to find this library, by appending
to the Linux environment variable \textbf{PYTHONPATH}. If you are
using Python version {}``py\_vers'', then the NumPy libraries will
have been installed in the directory {}``np\_dir/lib/py\_vers/site-packages'',
or a close analogue of this. In the above case the following command
will allow the Python interpreter to find the NumPy libraries:

\begin{verbatim}
> export PYTHONPATH=$PYTHONPATH:np_dir/lib/py_vers/site-packages
\end{verbatim}

Now Python scripts can import the NumPy libraries using:

\begin{verbatim}
import numpy
\end{verbatim}


\subsection{PyFFTW}

Some of the steps in the dynamics algorithm involve a change of variables
from the bead coordinates to the normal modes of the ring polymers.
Currently, this transformation is, at least by default, computed using
a fast-Fourier transform (FFT) library within the NumPy distribution.
This however is not the only distribution that could be used, and
indeed faster stand-alone versions exist. The gold-standard FFT library
is the FFTW library, which is a set of C libraries that have been
heavily optimized for a wide range of applications. There have been
a number of Python wrappers around the FFTW library, one of which
is currently interfaced with \ipi. This code can be found at \url{https://github.com/hgomersall/pyFFTW},
and has documentation at \url{http://hgomersall.github.io/pyFFTW/}.

This code has the following dependencies:
\begin{itemize}
\item Python version 2.7 or greater
\item Numpy version 1.6 or greater
\item FFTW version 3.2 or greater
\end{itemize}
This can be installed in the same way as NumPy, except using the code
distribution above, or using various installation packages as per
the instructions on the above documentation. Note that no other options
need to be specified in the input file, the wrapper will check to
see if this library is available, and if it is it will be used by
default. Otherwise the slower NumPy version will be used.


\section{Clients}




\subsection{Minimal client code}


\subsection{Patching CP2K}


\subsection{Patching Quantum-Espresso}

You can download the source code for Quantum Espresso at \url{www.quantum-espresso.org/}.
The tar file can be extracted in the same way as the Python and NumPy
libraries above. However, before this can run with \ipi, the code
must be adapted to use the socket interface. For Quantum Espresso
version 4.3.2, there is a patch file to allow it to be used as a client
code with \ipi in the directory {}``patches/pwscf''. If you are
currently in the top level directory of the Quantum-Espresso distribution,
the patch can be applied to the source code using:

\begin{verbatim}
> patch -p1 < i-pi/patches/pwscf/pw-driver.patch
\end{verbatim}

After this, continue the compilation as per the instructions at \url{www.quantum-espress.org/}.


\subsection{Writing a patch}


\section{Running \ipi}


\subsection{Running the server code}

\ipi simulations are run using the i-pi Python script found in the
{}``i-pi'' directory. This script takes an xml-formatted file as
input, and automatically starts a simulation as specified by the data
held in it. If the input file is called {}``input\_file.xml'', then
the wrapper is run using:

\begin{verbatim}
> python i-pi input_file.xml
\end{verbatim}

This reads in the input data, initializes all the internally used
objects, and then creates the server socket. The code will then wait
until at least one client code has connected to the server before
running any dynamics. Note that until this has happened the code is
essentially idle, the only process that it runs is to periodically
poll for incoming connections.


\subsection{Running the client code}

\label{runningclients}


\subsubsection{CP2K}

To use CP2K as the client code, an additional file named {}``serverfile''
must be kept in the same directory that CP2K is being run from. It
should contain a string a single line long, of the format {}``mode:host:port\_number''.
{}``mode'' should either be {}``INET'' or {}``UNIX'', representing
an internet or unix socket respectively. The rest of the input file
is the same as for a standard CP2K calculation.


\subsubsection{Quantum-Espresso}

To use Quantum-Espresso as the client code using a socket on the host
address {}``host\_address'' and on the port number {}``port''
we would add the following lines to the input file:

\begin{verbatim}
&CONTROL
   ...
   calculation=`driver'
   srvaddress=`host_address:port'
   ...
/
\end{verbatim}

If we wanted to run on a UNIX port instead of an INET port, we would
instead write:

\begin{verbatim}
&CONTROL
   ...
   calculation=`driver'
   srvaddress=`UNIX:host_address:port'
   ...
/
\end{verbatim}

The rest of the input file should be the same as for a standard Quantum
Espresso calculation, as explained at \url{www.quantum-espress.org/}.


\subsection{Running simulations}

\label{runningsimulations}

There are two parts to any \ipi simulation. First the server code
is run, which starts the dynamics loop until the forces are required.
Once the server socket has been opened, it then waits for connections
from client codes. 

For a small simulation, with a simple classical potential, it may
be preferable to run both halves of the simulation on the same computer,
especially as this allows us to use the faster unix domain sockets.
This can be done using a single script, such as:

\begin{verbatim}
#These are the parameters that need to be specified by
#the user. The number of client codes is given by $nclients,
#the i-pi source directory containing the i-pi script
#is given by $src_dir, and the code that runs one of 
#the client codes is given by $client_comm
inputfile=...
hostname=...
port=...
nclients=...
src_dir=...
client_comm="..."

#This modifies the input file. If necessary, the client code
#input file should be modified here too
sed -i "s/<address>[^<]*</<address>$hostname</" $inputfile
sed -i "s/<port>[^<]*</<port>$port</" $inputfile

if [ -e EXIT ]; then
   rm EXIT 
fi

#Runs the simulation, and redirects the server
#standard output to a file named 'log'.
bash -c "python $src_dir/i-pi $inputfile > log &"

sleep 20 #must wait for server to be initialized
for a in `seq 1 $nclients`; do
   bash -c "$client_comm"
done 
\end{verbatim}

However, for most simulations, especially those running with expensive
\emph{ab initio} forces and potentials, we will want to run the client
codes on a computational cluster. Here we run the server code elsewhere,
and use internet sockets for the communication.

To run the client on a cluster computer, two major changes must be
implemented. Firstly, the code required to run the client code should
be separated from that of the server code. Secondly, the client script
should be executed using a queueing program such as qsub rather than
the bash shell script as above, and so will need to have the appropriate
directive comment lines. Depending on which cluster is used, additional
commands to set parameters such as the walltime, number of nodes and
number of cores can be used. Check the qsub man page for the cluster
you are running the code on for more details.

One possible script layout to run the client codes would be:

\begin{verbatim}
#!/bin/bash
#$ -S /bin/bash
#$ -N jobname
#$ ...

#use this if there are library files needed
#by the client code that are not on the default path

#export LD_LIBRARY_PATH=...

#use this if ssh tunnelling is required

#server_host=... #the server host address
#server_port=... #the server port number
#cluster_port=... #the cluster port number
#ssh -f -N $server_host -L $cluster_port:$server_host:$server_port

inputfile=...
hostname=...
port=...
client_comm="..."

#Modify client input file if necessary

bash -c "client_comm"
\end{verbatim}

Note that it may be necessary to export libraries or modules if they
are not present by default, or to use an ssh tunnel if the cluster
is not open to the internet (see \ref{ssh_sockets}).

To set up multiple concurrent \ipi simulations each host socket has
to be distinguishable, so that the client codes appropriate to each
simulation connect only to the correct server. This can be done with
internet sockets by adjusting the port number for each simulation,
which then acts as a unique identifier for each of the server codes.
The above shell scripts can be easily modified to do this, for example
if we assume the simulations will be run in directories called sim\_1,
sim\_2, \ldots, then we can write:

\begin{verbatim}
first_port=... #The value of the port of sim_1
i=0
for a in sim_*; do #finds all directories in which we wish to run simulations
   cd $a
   port=$((first_port+i))
   ((i++))

   #here we run the code for one simulation as before

   cd ..
done
\end{verbatim}


\section{A simple tutorial}



Here we give a simple step-by-step guide through an example
simulation, exploring
some of the more generally useful options that \ipi offers and making
no assumptions of previous experience of this code or other MD codes.
Exercepts from the relevant input files are reproduced here, for explanation
purposes, but to get the most out of this tutorial the user is encouraged
to work through them themselves. For this purpose, the input files
have been included with the \ipi distribution, in the {}``test/tutorial''
directory.

The chosen problem is that of a small NPT simulation of para-hydrogen, 
using the Silvera-Goldman potential \cite{silv-gold78jcp}. 
We will take (N,P,T) = (108, 0, 25 K).

Finally, note that this is designed to be a demonstration of some
of the basic abilities of \ipi, and so the user is encouraged to
play with some of the variables to get a feel for how the input file
works. \textbf{We will put in bold variables that can be safely adjusted without
changing the result of the simulation.}


\subsection{Part 0 - a minimal input file}


\subsubsection{Client code}

Let us now consider the problem of how to use \ipi to run a NPT
simulation of para-hydrogen. The first thing that is required is a
client code that is capable of calculating the potential interactions
of para-hydrogen molecules. Fortunately, one of the client codes distributed
with \ipi has an appropriate empirical potential already hard-coded
into it (hence why this system was chosen for this tutorial), and
so all that is required is to create the {}``driver.x'' file in
the {}``drivers'' directory, using the UNIX utility make.

This client code can be used for several different problems, some of which
are explored in the {}``tests'' directory, but for the current problem
we will use the Silvera-Goldman potential with a cut-off radius
of \textbf{15 bohr radii}. This is run using the following command:

\begin{verbatim}
> ./driver.x -m sg -u -h localhost -o 15
\end{verbatim}

The option {}``-m'' is followed by the empirical potential required,
in this case we use {}``sg'' for Silvera-Goldman, {}``-u'' gives
a unix domain socket, {}``-h localhost'' sets up the client hostname
as {}``localhost'' and {}``-o 15'' sets the cut-off to 15 bohr
radii, as required. 

Note that usually this step will require setting up an the appropriate
client code input files, possibly for an \emph{ab initio} electronic
structure code, and so is generally a more involved process. Refer
to \ref{runningclients}, and the documentation of the appropriate
client code, for more details on how to do this step.


\subsubsection{Creating the xml input file}

Now that the client code is ready, an appropriate xml input file needs
to be created from which the host server and the simulation data can
be initialized. Here, we will go step by step through the creation
of a minimal input file for a simple NVT equilibration run. Note that
the working final version is held within the {}``tutorial-0'' directory.

Firstly, when reading the input file the \ipi xml reading functions look
for a {}``simulation''
tag as a sign to start reading data. For those familiar with xml jargon,
we have defined {}``simulation'' as the root tag, so all the input
data read in must start and end with a {}``simulation'' tag, as
show below:

\begin{verbatim}
<simulation>
   Input data here...
</simulation>
\end{verbatim}

xml syntax requires a set of hierarchially nested tags, each of which
contain data and/or more tags. Also, \ipi itself requires certain
tags to be present, and keeps track of which tags are supposed to
be where. More information about which tags are available can be found
in \ref{hierarchy}, more information on xml syntax can be found in
\ref{ifilestructure}, and possible errors which can occur if the
input file is not well formed can be found in \ref{trouble}. For
the sake of this first tutorial however, we will simply discuss the
mandatory tags: {}``initialize'', {}``forces'' and {}``ensemble''.
These correspond to the tag to initialize the atom configurations,
the tag to specify the client code and the tag to define the appropriate
ensemble respectively.

At this point then, the input file looks like:

\begin{verbatim}
<simulation>
   <initialize>
      ...
   </initialize>
   <forces>
      ...
   </forces>
   <ensemble>
      ...
   </ensemble>
</simulation>
\end{verbatim}

Now let us consider each of these tags in turn. Firstly, {}``initialize''.
As the name suggests, this initializes the state of the system, so
this is where we will specify the atom positions and the cell
parameters. Firstly, this takes an attribute which specifies the number
of replicas of the system, called {}``nbeads''. An attribute is
a particular type of xml syntax designed to specify a single bit of
data, and has the following syntax:

\begin{verbatim}
<initialize nbeads='4'>
   ...
</initialize>
\end{verbatim}

Note that an attribute forms part of the opening tag, and that the
value being assigned to it is held within quotation marks. In this
case, we have set the number of replicas, or beads, to \textbf{4}.

Next, we must specify the atomic configuration. Rather than initialize
the atom positions manually, we will assume that the user can create
a xyz file with the appropriate data. This is the simplest input format
for a configuration file that \ipi accepts, and it has the following
syntax:

\begin{verbatim}
natoms
# COMMENT LINE: PUT TITLE OF FILE HERE
atom1   x1  y1  z1
atom2   x2  y2  z2
...
\end{verbatim}

Where {}``natoms'' is replaced by an integer giving the total number
of atoms, in this case 108, atom1 is a label for atom 1, in this case
H2 (since we are simulating para-hydrogen), and (x1, y1, z1) are the
x, y and z components of atom 1 respectively. Note that this file
is free-formatted, and so the precision of each of the position coordinates
is arbitrary. Also note that visualization software such as VMD will
be able to read this file format, and so it is generally advised to
use such software to make sure that the configuration is as expected.
For the sake of this tutorial, we have included a valid xyz input
file in the {}``tutorial-0'' directory called {}``our\_ref.xyz''.

To use this reference file, we will use the {}``file'' tag in initialize.
This will take an input file with a given name, and use it to initialize
all relevant data. In this case, since the xyz file format has both
positions and atom labels, it will initialize the positions, labels
and masses of all the particles in the system, with the masses being
implicitly set based on the atom label. We must also tell it which
file type is being used using the {}``mode'' attribute. Putting
this together gives:

\begin{verbatim}
<initialize nbeads='4'>
   <file mode='xyz'> our_ref.xyz </file>
   ...
</initialize>
\end{verbatim}

The only remaining mandatory parameters that have not been initialized
are the cell parameters. These could in theory be set using a separate
file, but here we will initialize them manually. Taking a cubic cell
with cell parameter 33.72594 bohr radii, we can specify this using
the {}``cell'' tag in three different ways:

\begin{verbatim}
<cell mode='manual'> 
   [33.72594, 0, 0, 0, 33.72594, 0, 0, 0, 33.72594] 
</cell>
\end{verbatim}

\begin{verbatim}
<cell mode='abcABC'>
   [33.72594, 33.72594, 33.72594, 90, 90, 90]
</cell>
\end{verbatim}

\begin{verbatim}
<cell mode='abc'>
   [33.72594, 33.72594, 33.72594]
</cell>
\end{verbatim}

Note the use of the different {}``mode'' attributes, {}``manual'',
{}``abcABC'' and {}``abc''. The first creates the cell vector
matrix manually, the second takes the length of the three unit vectors
and the angles between them in degrees, and the last assumes an orthorhombic
cell and so only takes the length of the three unit vectors as arguments.
We will take the last version for brevity, giving as our final {}``initialize''
section:

\begin{verbatim}
<initialize nbeads='4'>
   <file mode='xyz'> our_ref.xyz </file>
   <cell mode='abc'>
      [33.72594, 33.72594, 33.72594]
   </cell>
</initialize>
\end{verbatim}

Next lets consider the {}``forces'' section, which deals with communication
with the client codes. Since in this case we only have one type of
client code which will be using sockets for communication, we will
specify a single {}``sockets'' tag to initialize it:

\begin{verbatim}
<forces>
   <socket>
      ...
   </socket>
</forces>
\end{verbatim}

A socket is specified with three parameters; the port number, the
hostname and whether it is a unix or an internet socket. Here for
simplicity (and to match up with the client socket specified above)
we will take a unix socket, which uses the hostname localhost and
does not need a port number to be specified. This gives the final
{}``forces'' section:

\begin{verbatim}
<forces>
   <socket mode="unix">
      <hostname> localhost </hostname>
   </socket>
</forces>
\end{verbatim}

The last section that we will need is the ensemble, which determines
how the dynamics integrator will be initialized. Since we wish to
do a NVT simulation, we set the {}``mode'' attribute to {}``nvt'',
and must specify the temperature using the appropriate tag:

\begin{verbatim}
<ensemble mode='nvt'>
   <temperature> 25 </temperature>
   ...
</ensemble>
\end{verbatim}

This defines the ensemble that will be sampled. We also must decide
which integration algorithm to use, and how large the time step should
be. In general, the time step should be made as large as possible
without there being a drift in the conserved quantity. Usually we
would take a few short runs with different time steps to try and optimize
this, but for the sake of this tutorial we will use a safe value of
\textbf{40 atomic time units}, giving:

\begin{verbatim}
<ensemble mode='nvt'>
   <temperature> 25 </temperature>
   <timestep> 40 </timestep>
   ...
</ensemble>
\end{verbatim}

Finally, while the free-particle evolution part of the integrator
is initialized automatically, there are several different options
for the constant temperature sampling algorithm. For simplicity we
will take the path-integral Langevin equation (PILE) algorithm \cite{ceri+10jcp},
which is specifically designed for path integral simulations, and
since we are trying to equilibrate local properties we will take the
local version of this, \textbf{{}``pile\_l''}. This integrator also has to
be initialized with a time scale parameter, {}``tau'', which determines
how strong the thermostat is. Since we have a local thermostat the
appropriate time scales will be fairly short, so we will take a short
time scale of \textbf{1000 atomic time units}. Putting all of this together,
we get the final input file:

\begin{verbatim}
<simulation>
   <initialize nbeads='4'>
      <file mode='xyz'> our_ref.xyz </file>
      <cell mode='abc'>
         [33.72594, 33.72594, 33.72594]
      </cell>
   </initialize>
   <forces>
      <socket mode="unix">
         <hostname> localhost </hostname>
      </socket>
   </forces>
   <ensemble mode='nvt'>
      <temperature> 25 </temperature>
      <timestep> 40 </timestep>
      <thermostat mode='pile_l'>
         <tau> 1e3 </tau>
      </thermostat>
   </ensemble>
</simulation>
\end{verbatim}


\subsubsection{Running the simulation}

\label{run1}

Now that we have a valid input file, we can run the test simulation.
The {}``i-pi'' script in the root directory is used to create an
\ipi simulation from a xml input file. More information on how to
run this code can be found in \ref{runningsimulations}, but all that
is required in this case (if we assume that we are in the {}``tutorial-0''
directory) is:

\begin{verbatim}
> python ../../../i-pi tutorial-0.xml
\end{verbatim}

This will start the \ipi simulation, creating the server socket and
initializing the simulation data. This should at this point print
out a header message to standard output, followed by a few information
messages that end with {}``starting the polling thread main loop'',
which signifies that the server socket has been opened and is waiting
for connections from client codes.

At this point the driver code is run in a new terminal 
from the {}``drivers'' directory using the command
specified above:

\begin{verbatim}
> ./driver.x -m sg -u -h localhost -o 15
\end{verbatim}

The wrapper code should now output a message saying that a new client
code has connected, and start running the simulation.


\subsubsection{Output data}

Once the simulation is finished (which should take about a minute)
it should have output, by default, one file called {}``i-pi.checkpoint''
and one called {}``RESTART'' which have the state of the system
saved as it was at the end of the calculation, one file called {}``i-pi.md''
which contains a log of the values of some of the simple properties
of the system throughout the simulation, and a set of files {}``i-pi.pos\_x.xyz''
which give the positions of the different replicas of the system at
regular intervals. For an in-depth discussion on these three types
of output files see \ref{outputfiles}, but for now let us look at
each of these specific output files in turn.

First, consider the file {}``i-pi.checkpoint''. As said above, this
gives a snapshot of the state of the simulation. Since we didn't specify
how often to print out this file the default value of once every 1000
time steps was used, so in this case the last checkpoint was created
at step 999. This type of file is designed to be a way of restarting
the simulation, by using it as an input file for a new \ipi simulation. 
This is done in exactly the same way as before, i.e.:

\begin{verbatim}
> python ../../../i-pi i-pi.checkpoint
\end{verbatim}

The RESTART file is the same type of file, except this is automatically
generated at the end of the simulation regardless of what is
specified in the input file.

The next set of files to consider are the trajectory files
{}``i-pi.pos\_x.xyz''. Trajectory files are used to print out properties
relevant to all the atoms individually, such as the velocities or forces.
In this case, these give the positions of the 
beads at various snapshots throughout the simulation. These can be useful
for calculating correlation functions or radial distribution functions,
but possibly their most useful feature is that visualization
programs such as VMD can read them, and then use the data to show
a movie of how the simulation is progressing. 

If we do this with these files, we see that the simulation started
from an essentially optimized configuration and then over the course
of the simulation began to melt. Given that at the state point studied
and with the potential given para-hydrogen is a liquid \cite{silv-gold78jcp},
this is what we would expect.

Finally, let us consider the {}``i-pi.md'' file. This is a print out
of the system level properties, and by default we get one file which
prints out the timestep, elapsed time, conserved quantity, temperature,
potential energy and kinetic energy of the system.

At this point there are two simple tests that should be done. Firstly,
we should check that the conserved quantity does not exhibit any major
drift, and second we should check to see if the properties of interest
have converged yet. The file format has been chosen so that programs 
such as awk and gnuplot can read the data easily. In this case, gnuplot
can show how the conserved quantity, temperature, potential energy
and kinetic energy change throughout the simulation using:

\begin{verbatim}
> gnuplot
> p './i-pi.md' u 1:3 # Plots column 1, i.e. timestep, 
> p './i-pi.md' u 1:4 # against columns 3, 4, 5 and 6,
> p './i-pi.md' u 1:5 # i.e. conserved quantity, temperature,
> p './i-pi.md' u 1:6 # kinetic energy and potential energy
\end{verbatim}

This will show that the conserved quantity has only a small drift upwards,
so the time step is sufficiently small, and that the temperature has converged
but the kinetic and potential energies have not.
We therefore need to adjust the input parameters to converge these properties
properly, as will be done in the next section.

\subsection{Part 1 - Customizing and extending the input file}

In Part 0, we created a minimal input file which could be used to
run a basic NVT simulation. However, this ignored several non-essential
but very useful bits of functionality, which are likely to be 
important for most problems users will face. With this in mind,
we now present some impovements to the original input file. 

\subsubsection{Improving convergence and simulation technicalities}

Firstly, we will discuss the input parameters that are necessary for
the simulation to actually do what we originally intended it to
correctly, namely equilibate a small system in the NVT ensemble.

We noted at the end of the last run that the simulation did not appear to have
fully converged. Often in this case it will be necessary to adjust the parameters
of the integrators, such as the time step and the thermostat timescale.
However, in this case there are a couple of much simpler ways to
help improve the convergence which were neglected previously, so we will now
concentrate on these.

The most obvious first step is to run the simulation for longer. 
By default the number of
time steps is 1000, so we will increase this to \textbf{5000} using the 
{}``total\_steps'' tag:

\begin{verbatim}
<total_steps> 5000 </total_steps>
\end{verbatim} 

Another thing we can do to increase the initial convergence of the energy 
is to initialize the atom velocities. Rather than setting these manually,
the simplest method is to sample these randomly from a Maxwell-Boltzmann
distribution. This can be done in the {}``initialize'' section using the
{}``velocities'' tag:

\begin{verbatim}
<initialize nbeads='4'>
   <file mode='xyz'> our_ref.xyz </file>
   <cell mode='abc'>
      [33.72594, 33.72594, 33.72594]
   </cell>
   <velocities mode='thermal'> 50 </velocities>
</initialize>
\end{verbatim}

Note that we use \textbf{double the physical temperature} 
here, since we are starting from essentially a zero-temperature optimized 
configuration and so this choice ensures that the initial total energy 
is roughly correct.

One small technicality that can become important if multiple similar
simulations are run is that, to ensure each run is independent, the 
random number generator should be initialized with different
seeds for each. This is simply done using the
{}``prng'' tag, as follows:

\begin{verbatim}
<prng>
   <seed>
      31415
   </seed>
</prng>
\end{verbatim}

\subsubsection{Simplifying the input file}

Some of the functionality of \ipi is designed to make it easier to use rather
than to improve the performance of the code. One example of this is the
unit conversion library. If appropriate, the input classes have a {}``dimension''
parameter which is used to define their dimensionality.
These variables can then be specified with a {}``units'' attribute
in the input file, which
the xml reading functions can use to automically convert between a 
user specified unit and atomic units. 
For example, the time step we used in tutorial 0
was 40 atomic time units, but we might instead want to express this in S.I.
units for comparison with another paper. Since this happens to be approximately
\textbf{1 femtosecond}, so we can instead use:

\begin{verbatim}
<ensemble mode='nvt'>
   <timestep units='femtosecond'> 1 </timestep>
   ...
</ensemble>
\end{verbatim}
 
Similarly, we might want to specify the input configuration file in
angstrom rather than bohr radii (in fact this is the standard unit for
this file type). This can be done in the same way:

\begin{verbatim}
<initialize nbeads='4'>
   <file mode='xyz' units='angstrom'> our_ref.xyz </file>
   <cell mode='abc' units='angstrom'>
      [33.72594, 33.72594, 33.72594]
   </cell>
   <velocities mode='thermal'> 50 </velocities>
</initialize>
\end{verbatim}

This {}``initialize'' section can also be further simplified by changing the 
configuration file type to one that can also be used to specify the cell
parameters. Currently, the only file type which can do this that is 
compatible with \ipi is the pdb file type, which has the following structure:

\footnotesize{
\begin{verbatim}
TITLE insert title here...
CRYST1        a        b        c      A      B      C P 1           1
ATOM      1   n1   1     1          x1      y1      z1  0.00  0.00             0
ATOM      2   n2   1     1          x2      y2      z2  0.00  0.00             0
...
\end{verbatim}
}
\normalsize
where a, b and c are the cell vector lengths, A, B and C are the angles between
them, n1 and n2 are the labels for atoms 1 and 2, and (x1, y1, z1)
and (x2, y2, z2) give the position vectors of atoms 1 and 2. Note that this is
fixed-formatted, so the number of spaces matters. For more information see
\url{http://deposit.rcsb.org/adit/docs/pdb_atom_format.html}

Using this, the initialize section is reduced to:

\begin{verbatim}
<initialize nbeads='4'>
   <file mode='pdb' units='angstrom'> our_ref.pdb </file>
   <velocities mode='thermal'> 50 </velocities>
</initialize>
\end{verbatim}

Note that an appropriately formatted pdb file is included with the
{}``tutorial 1'' directory as before.

\subsubsection{Customizing the output}

In the first tutorial only the default output was generated since 
nothing had been specified by the user. There are however a wide
variety of properties of interest that \ipi can calculate 
and a large number of different output
options, so in most cases you will want to specify what is output
manually.

Firstly, the amount of data sent to standard output can be adjusted
with the {}``verbosity'' attribute of {}``simulation'':

\begin{verbatim}
<simulation verbosity='high'>
   ...
</simulation>
\end{verbatim}

By default the verbosity is set to {}``low'', which only outputs
important warning messages and information, and some statistical 
information every 1000 time steps. Here we will set it to 
\textbf{{}``high''}, which should give output of the type:

\small
\begin{verbatim}
 # Average timings at MD step S. t/step: TOTAL [p: P q: Q t: T]
 # MD diagnostics: V: POTENTIAL Kcv: KINETIC Ecns: CONSERVED
 @SOCKET: Assigning [ X ] request id ID to client with last-id LID ( CID/ CTOT : )
\end{verbatim}
\normalsize
where the output values have been replaced with the following:
\begin{description}
\item [{S:}] This gives the current time step.
\item[{TOTAL:}] This gives the amount of time the current time step took.
\item [{P:}] This gives how long the momentum propagation step took.
\item [{Q:}] This gives how long the free-ring polymer propagation step took.
\item [{T:}] This gives how long the thermostat integration step took.
\item [{POTENTIAL:}] This gives the current potential energy of the system.
\item [{KINETIC:}] This gives the current kinetic energy of the system.
\item [{CONSERVED:}] This gives the current conserved quantity.
\item [{X:}] This says whether or not \ipi found a match for the calculation of 
replica ID or not.
If one of the connected client codes calculated the forces for this replica on the
last time step, then X will be {}``match'', and \ipi will automatically assign
this replica to the same client as before. This should happen with all the replicas
if CTOT is the same as the number of beads.
\item [{ID:}] The index of the replica currently being assigned to a client code.
\item [{LID:}] The index of the replica which the client code last did a force calculation
of.
\item [{CID:}] The index of the client code in the list of all connected client codes.
\item [{CTOT:}] The total number of connected client codes.
\end{description}

What output gets written to file is specified by the {}``output'' tag.
There are three types of files, as explained in tutorial 0, 
properties files, trajectory files and checkpoint files, which 
are specified with {}``properties'', {}``trajectory'' 
and {}``checkpoint'' tags respectively.
As an example, the default output would be generated with
the following {}``output'' section:

\begin{verbatim}
<output prefix='i-pi'>
   <properties filename='md' stride='10'>
      [time, step, conserved, temperature, potential, kinetic_cv]
   </properties>
   <trajectory filename='pos' stride='100' format='xyz'>
      positions
   </trajectory>
   <checkpoint filename='checkpoint' stride='1000' overwrite='True'/>
</output>
\end{verbatim}

As seen in tutorial 0, this creates 6 files: {}``i-pi.md'', {}``i-pi.pos\_0.xyz'',
{}``i-pi.pos\_1.xyz'', {}``i-pi.pos\_2.xyz'', {}``i-pi.pos\_3.xyz''
and {}``i-pi.checkpoint''. 
The filenames are created using the syntax 
{}``prefix''.{}``filename''[\_(file specifier)][.(file format)], where the file specifier is
added to separate similar files. For example, in the above case the 
different position trajectories for each bead are given a file specifier
corresponding to the appropriate bead index.

The {}``stride'' attributes set how often data is output to each file;
so in the above case the properties are written out every 10 time steps,
the trajectories every 100, and the checkpoints every 1000.
The {}``format'' attribute sets the format of the trajectory files,
and the {}``overwrite'' attribute sets whether each checkpoint file 
overwrites the previous one or not.

There are several options we can use to customize the
output data. Firstly, the {}``prefix'' attribute should be set to
something which can be used to distinguish the files from different
simulation runs. In this case we can simply set it to \textbf{{}``tut1''}:

\begin{verbatim}
<output prefix='tut1'>
   ...
</output>
\end{verbatim}

As for the input parameters, the \textbf{units the output is given in} can be
set by the user. Unlike the input parameters however, 
this is done by specifying an
appropriate unit in curly braces after the name of the 
property or trajectory of interest, as shown below:

\begin{verbatim}
<output prefix='tut1'>
   <properties filename='md' stride='10'>
      [step, time{picosecond}, conserved{kelvin}, 
       temperature, potential{kelvin}, kinetic_cv{kelvin}] 
   </properties>
   <trajectory filename='pos' stride='100' format='xyz'>
      positions{angstrom} 
   </trajectory>
   <checkpoint filename='checkpoint' stride='1000' overwrite='True'/>
</output>
\end{verbatim}

Next, let us adjust some of the attributes. Let us suppose that we
wish to output the properties \textbf{every time step}, to check for conserved
quantity jumps, and to output the the trajectory in \textbf{pdb format}.
To do this we would set the {}``stride'' and {}``format'' tags,
as shown below:

\small
\begin{verbatim}
<output prefix='tut1'>
   <properties filename='md' stride='1'>
      [step, time{picosecond}, conserved{kelvin}, 
       temperature, potential{kelvin}, kinetic_cv{kelvin}] 
   </properties>
   <trajectory filename='pos' stride='100' format='pdb' cell_units='angstrom'>
      positions{angstrom} 
   </trajectory>
   <checkpoint filename='checkpoint' stride='1000' overwrite='True'/>
</output>
\end{verbatim}
\normalsize

Note that we have added a {}``cell\_units'' attribute
to the {}``trajectory'' tag, so that the
cell parameters are consistent with the position output.

Finally, let us suppose that we wished to output another output property
to a different file to the others. One example of when this 
might be necessary if there
is one output property which was more expensive to calculate than the
others, and so it would be impractical to output it every time step.
With \ipi this is easy to do, all that is required is to add another
{}``properties'' tag with a different filename.

For demonstration purposes, we will chose to print out \textbf{the forces
acting on one tagged bead}, since this requires an argument to be
passed to the function. The \ipi syntax for doing this is to have
the arguments to be passed to the function between standard braces,
separated by semi-colons.

To print out the forces acting on one bead we need the {}``atom\_f''
property, which takes two arguments, {}``atom'' and {}``bead'',
giving the index of the atom and bead tagged respectively. The
appropriate syntax is then given below:

\begin{verbatim}
<properties> 
   [atom_f(atom=0;bead=0)] 
</properties>
\end{verbatim}

This will print out the force vector acting on \textbf{bead 0 of atom 0}.
\ipi also accepts positional arguments
(i.e. arguments not specified by a name, but just by their position
in the list of arguments), and so this could also be written as:

\begin{verbatim}
<properties> 
   [atom_f(0;0)] 
</properties>
\end{verbatim}

Finally, putting all this together, and \textbf{adjusting some
of the parameters of the new file}, we get:

\small
\begin{verbatim}
<output prefix='tut1'>
   <properties filename='md' stride='1'> 
      [step, time{picosecond}, conserved{kelvin}, 
       temperature, potential{kelvin}, kinetic_cv{kelvin}] 
   </properties>
   <properties filename='force' stride='20'> 
      [atom_f{piconewton}(atom=0;bead=0)] 
   </properties>
   <trajectory filename='pos' stride='100' format='pdb' cell_units='angstrom'> 
      positions{angstrom} 
   </trajectory>
   <checkpoint filename='checkpoint' stride='1000' overwrite='True'/>
</output>
\end{verbatim}
\normalsize

\subsubsection{Equilibration run}

Now that we have adjusted the input file to our satisfaction, we can
re-run the simulation by 
repeating the steps in \ref{run1}. The output of the 
{}``tut1.md'' file should show that all the properties of interest
have converged by the end of the simulation. If this
is the case, then we are now ready to start the NPT run.

\subsection{Part 2 - NPT simulation}

Now that we have converged NVT simulation data, we can use this to
initialize a NPT simulation. There are two ways of doing this,
both of which involve using the RESTART file generated at
the end of the NVT run as a starting point.

\subsubsection{Modifying the RESTART file}

Firstly, you can use the RESTART file directly, modifying it
so that instead of continuing with the original NVT simulation
it will instead it will start a new NPT simulation. We have included
in the {}``tutorial 2'' directory both a RESTART file from
tutorial 1 and an adjusted file which will run NPT dynamics.

These adjustments start with resetting the {}``step'' tag, so that
it starts with the value 0. This can be done by simply removing the
tag. Similarly, we can increase the total number of
steps so that it is more suitable for collecting the necessary
amount of data, in this case we will set the 
{}``total\_steps'' to \textbf{20000}.

We will also update the output files, first by setting the filenames
to start with {}``tut2a'' rather than {}``tut1'', and secondly by adding
the volume and pressure to the list of computed propertie so that
we can check that the ensemble is being sampled correctly.
Putting this together this gives:

\small
\begin{verbatim}
<output prefix=''>
   <properties shape='(8)' filename='tut2a.prop'>
      [ step, time{picosecond}, conserved{kelvin}, 
        temperature, potential{kelvin}, kinetic_cv{kelvin}, 
        pressure_cv{megapascal}, volume ] 
   </properties>
   <properties stride='20' shape='(1)' filename='tut2a.force'> 
      [ atom_f{piconewton}(atom=0;bead=0) ] 
   </properties>
   <trajectory filename='tut2a.pos' stride='100' format='pdb' cell_units='angstrom'>
      positions{angstrom}
   </trajectory>
   <checkpoint stride='1000' filename='tut2a.restart'/>
</output>
\end{verbatim}
\normalsize

Finally, we must change the {}``ensemble'' tag so that the correct
ensemble is sampled. The first thing that must be done is  
the {}``mode'' tag must be changed from {}``nvt'' to {}``npt'',
and a {}``pressure'' tag must be added:

\begin{verbatim}
<ensemble mode='npt'>
   <pressure> 0 <pressure>
   ...
</ensemble>
\end{verbatim}

We must also specify which constant pressure algorithms are to be used.
\ipi contains two barostats, labelled {}``mht'' and {}``bzp''. These are
broadly similar, but use slightly different integration algorithms.
In this case, we will use \textbf{{}``bzp''}. This gives:

\begin{verbatim}
<ensemble mode='npt'>
   <pressure> 0 <pressure>
   <barostat mode='bzp'>
      ...
   </barostat>
   ...
</ensemble>
\end{verbatim}

This barostat also requires a 
thermostat to deal with the volume degree of freedom, which we will take
to be a simple \textbf{Langevin thermostat}.
This thermostat is specified in the same way as the one which does the
constant temperature algorithms for the atomic degrees of freedom, and
we will take its time constant to be \textbf{10000 atomic time units}:

\begin{verbatim}
<ensemble mode='npt'>
   <pressure> 0 <pressure>
   <barostat mode='bzp'>
      <thermostat mode='langevin'>
         <tau> 1e4 </tau>
      </thermostat>
      ...
   </barostat>
   ...
</ensemble>
\end{verbatim}

Finally, we will take the barostat time scale to be \textbf{10000 atomic time
units}, giving:

\begin{verbatim}
<ensemble mode='npt'>
   <pressure> 0 <pressure>
   <barostat mode='bzp'>
      <thermostat mode='langevin'>
         <tau> 1e4 </tau>
      </thermostat>
      <tau> 1e4 </tau>
   </barostat>
   ...
</ensemble>
\end{verbatim}
with the rest of the {}``ensemble'' tag being the same as before.

\subsubsection{Initialization from RESTART}

A different way of initializing the simulation is to use the 
RESTART file as a configuration file, in the same way that
the xyz/pdb files were used previously.

Firstly, the original input file {}``tutorial-1.xml'' needs to
be modified so that
it will do a NPT simulation instead of NVT. This involves modifying
the {}``total\_steps'', {}``output'' and {}``ensemble'' tags as
above. Next, we replace the {}``initialize'' tag section with:

\begin{verbatim}
<initialize nbeads='4'>
   <file mode='chk'> tutorial-1_RESTART </file>
</initialize>
\end{verbatim}

Note that the {}``mode'' attribute has been set to {}``chk''
to specify that the file is a checkpoint file.
This will then use the RESTART file to initialize the bead
configurations and velocities and the cell parameters.

\subsubsection{Running the simulation}

Whichever method is used to create the input file, the simulation
is run in the same way as before. 
Note how the volume fluctuates with time, as it is no longer held
constant in this ensemble.

\subsection{Part 3 - A fully converged simulation}

As a final example, we note that at this state point 16 replicas
and at least 172 particles are actually 
required to provide converged results.
As a last tutorial then, you should repeat tutorials 1 and 2 with
this number of replicas and atoms. The directory {}``tutorial 3'' contains 
NVT and NPT input files which can be used to do a fully converged
NPT simulation from scratch, except that they are missing some
of the necessary input parameters.

If these are chosen correctly and the simulation is run properly
the volume will be 31 \(\textrm{cm}^3\)/mol and the total energy
should be -48 K \cite{mart+99jcp}.


\section{Testing the install}

\label{tests}

Several test cases are distributed with the code to ensure that your
distribution is working correctly. There are also simple tests to
see if the client codes are working correctly.

All the input files are contained in the directory test, which is
subdivided into the following directories:
\begin{description}
\item [{lj:}] This gives a simple classical Lennard-Jones simulation of
Ne. The state points are given by (N, \(\rho\), T) = (864, 0.35,
1.62), (N, \(\rho\), T) = (864, 0.75, 1.069) and (N, \(\rho\), T)
= (864, 0.88, 1.095) in reduced Lennard-Jones units, so that the results
can be compared to those in the paper \cite{lverlet67pr}.
\item [{ph2:}] This simulates para-hydrogen using the isotropic Silvera-Goldman
pair potential. There are three directories, {}``RPMD'', {}``nvt''
and {}``Tuckerman''. {}``RPMD'' and {}``nvt'' have tests which
can be compared to the results of \cite{mill-mano05jcp}, and {}``Tuckerman''
has tests which can be compared to the results of \cite{mart+99jcp}.
\item [{pwscf:}] This has two simple examples to test to see if the Quantum-Espresso
client is functioning correctly. There is one simple 4-atom lithium
test, and a test using a single water molecule.
\item [{harmonic:}] This has a simple example of a 1D harmonic oscillator.
This demonstrates the displaced path integral momentum distribution
operator as given in \cite{linlin+10prl}. As the momentum distribution
is known analytically for this simple system, this provides an indication
of how well the method is working.
\item [{lammps:}] This has a simple implementation of the q-TIP4P-F empirical
water model of \cite{habe+09jcp} using the classical molecular dynamics
code LAMMPS. It demonstrates both the convergence of the PIGLET method
\cite{ceri-mano12prl}, as well as the use of ring-polymer contraction
methods \cite{mark-mano08jcp}.
\end{description}

\chapter{User guide}

\label{user}


\section{Input files}


\subsection{Input file format and structure}

\label{ifilestructure}

In order to give the clearest layout, xml formatting was chosen as
the basis for the for the main input file. An xml file consists of
a set of hierarchically nested tags. There are three parts to an xml
tag. Each tag is identified by a tag name, which specifies the class
or variable that is being initialized. Between the opening and closing
tags there may be some data, which may or may not contain other tags.
In the code this is used to specify the contents of a class object,
or the value of a variable. Finally, any tag can have attributes,
which in the code are used for variables which specify the state or
type of class that will be created. A xml tag has the following syntax:

\begin{verbatim}
<tag_name attribute_name='attribute_data'>tag_data</tag_name>
\end{verbatim}

The syntax for the different types of tag data is given below: 

\begin{tabular}{cc}
\hline 
Data type & Syntax\tabularnewline
\hline 
\hline 
Boolean & <tag>True</tag> or <tag>False</tag>\tabularnewline
Float & <tag>11.111</tag> or <tag>1.1111e+1</tag>\tabularnewline
Integer & <tag>12345</tag>\tabularnewline
String & <tag>string\_data</tag>\tabularnewline
Tuple & <tag> (int1, int2, \ldots )</tag>\tabularnewline
Vector & <tag> {[} entry1, entry2, \ldots {]} </tag>\tabularnewline
Dictionary & <tag>\{name1: data1, name2: data2, \ldots \}</tag>\tabularnewline
\hline 
\end{tabular}

Note that arrays are always given as one-dimensional lists. In cases
where a multi-dimensional array must be entered, one can use the `shape'
attribute, that determines how the list will be reshaped into a multi-dimensional
array; for instance

\begin{verbatim}
<tag shape=`(1,2,3)'>[ a111, a112, a113, a121, a122, a123 ]</tag>
\end{verbatim}If `shape' is not specified, a 1D array will be assumed.

The code uses this hierarchical structure to help read the data; if
a particular object is held within a parent object in the code, then
the tag for that object will be within the appropriate parent tags.
This is used to make the structure of the simulation clear. For example,
the system that is being studied is partly defined by the thermodynamic
ensemble that should be sampled, which in turn may be partly defined
by the pressure, and so on. In the input file this would be specified
by having a {}``simulation'' tag, containing an {}``ensemble''
tag, which itself contains a {}``pressure'' tag, which will contain
a float value corresponding to the external pressure. In the code
itself this will correspond to a simulation object, which will contain
an ensemble object, which will contain a pressure variable. In this
manner, the simulation class structure can be constructed iteratively.

For example, suppose we want to generate a NPT ensemble at an external
pressure of 1e-7 atomic pressure units. This would be specified by
the following input file: 

\begin{verbatim}
<simulation>
   <ensemble mode='npt'>
      <pressure> 1e-7 </pressure>
      ...
   </ensemble>
   ...
</simulation>
\end{verbatim}

To help detect any user error the recognized tag names, data types
and acceptable options are all specified in the code in a specialized
input class for each class of object. A full list of all the available
tags and a brief description of their function is given in chapter~\ref{hierarchy}.


\subsubsection{Overwriting units}

\label{inputunits}

Many of the input parameters, such as the pressure in the above example,
can be specified in more than one unit. Indeed, often the atomic unit
is inconvenient to use, and we would prefer something else. Let us
take the above example, but instead take an external pressure of 3
MPa. Instead of converting this to the atomic unit of pressure, it
is possible to use pascals directly using:

\begin{verbatim}
<simulation>
   <ensemble mode=`npt'>
      <pressure units=`pascal'> 3e6 </pressure>
      ...
   </ensemble>
   ...
</simulation>
\end{verbatim}

The code can also understand S.I. prefixes, so this can be simplified
further using:

\begin{verbatim}
<simulation>
   <ensemble mode='npt'>
      <pressure units=`megapascal'> 3 </pressure>
      ...
   </ensemble>
   ...
</simulation>
\end{verbatim}

A full list of which units are defined for which dimensionalities
can be found in the units.py module.


\subsection{Initialization section}


\subsubsection{Configuration files}

\label{configfile}

Instead of initializing the atom positions manually, the starting
configuration can be specified through a separate data file. The name
of the configuration file is specified within the {}``initialize''
tag name with the {}``mode'' attribute given by the file format.
The currently accepted file formats are:
\begin{itemize}
\item pdb
\item xyz
\end{itemize}
This file can be used either to initialize one variable only, or instead
to initialize multiple attributes by using the {}``file'' tag name.
This will initialize the atom positions, labels, masses and possibly
the cell parameters together.


\subsubsection{Initialization from checkpoint files}

\ipi gives the option to output the entire state of the system at
particular timesteps as an xml input file, called a checkpoint file
(see \ref{checkpoint} for details). As well as being a valid input
file, a checkpoint can also be used as an input data file in the same
way that the configuration files are. This is specified in the input
by using {}``chk'' as the value of the {}``mode'' attribute. As
for the configuration file, a checkpoint file can be used to initialize
either one or many variables depending on which tag name is used.

There is also an ensemble type, {}``replay'', which takes a configuration
file with multiple frames or a checkpoint file with one frame and
reruns it without doing dynamics, by simply setting the configuration
to match that given by the input file at each frame. This can then
be used to calculate properties of interest along a trajectory that
has already been finished, if the user forgot to calculate them the
first time around.


\section{Output files}

\label{outputfiles}

\ipi uses a very flexible mechanism to specify how and how often
atomic configurations and physical properties should be output. Within
the {}``output'' tag of the xml input file the user can specify
multiple tags, each one of which will correspond to a particular output
file. Each file is managed separately by the code, so what is output
to a particular file and how often can be adjusted independently of
the same parameters for a different file. It is also possible to have
multiple instances of the same type of file.

For example, some of the possible output properties require more than
one force evaluation per time step to calculate, and so can considerably
increase the computational cost of a simulation. On the other hand,
for properties such as the conserved energy quantity it is easy, and
often useful, to output them every time step as they are simple to
compute and do not take long to output to file. In this case it is
most efficient to only output the expensive property rarely, which
would be done by outputting this to a different file to the ones that
are being used to keep track of the conserved quantity, and use a
larger number of time steps between successive writes to this file.

There are three types of output file that can be specified; property
files for system level properties, trajectory files for atom/bead
level properties and checkpoint files which save the state of the
system and so can be used to restart the simulation from a particular
point. These will now be considered in turn.


\subsection{Properties}

\label{propertyfile}

This is the output file for all the system and simulation level properties,
such as the total energy and the time elapsed. The file starts by
a header, which describes the properties being written on the different
columns, and their units of measure. This is followed by the actual
data. Each line corresponds to one instant of the simulation, and
the different columns match the description provided in the header.
The file is fixed formatted, with two blank characters at the start
of each row, then the data in the same order as the header row. Each
column is 16 characters wide and every float is written in exponential
format with 8 digits after the decimal point.

The properties that are output are determined by the {}``properties''
tag in the xml input file. The format of this tag is:

\begin{verbatim}<properties stride=`' filename=`' flush=`' shape=`'>
   [ prop1name{units}(arg1; ... ), prop2name{...}(...), ...  ]
</properties>\end{verbatim}

The attributes have the following meanings:
\begin{description}
\item [{stride}] The number of steps between each output to file
\item [{filename}] The name of the output file
\item [{flush}] The number of steps between flushing the buffer
\end{description}
This tag data is an array of strings, each of which containing three
different parts:
\begin{itemize}
\item The property name, which describes which type of property is to be
output. This is a mandatory part of the string.
\item The units that the property will be output in. These are specified
between curly brackets. If this is not specified, then the property
will be output in atomic units.
\item The arguments to be passed to the function. These are specified between
standard brackets, with each argument separated by a semi-colon. These
may or may not be mandatory depending on the property. The arguments
can be specified by either of two different syntaxes, (name1=arg1;
\ldots ) or (arg1; \ldots ). The first explicitly assigns the argument
with the name {}``name1'' the value {}``arg1'', whereas the second
relies on the arguments being specified in the correct order, as defined
in the relevant function in the property.py module. The two syntaxes
may be mixed, but positional arguments must be specified first otherwise
undefined behaviour will result. If no arguments are specified, then
the defaults as defined in the properties.py module will be used.
\end{itemize}
The different available property names are:

\input{input_docs/property_list}


\subsection{Trajectory files}

These are the output file for atomic or bead level properties, such
as the bead positions. Each trajectory that should be output is specified
by the {}``trajectory'' tag in the input file. The allowable file
formats for the trajectory output files are the same as for the configuration
input files, given in~\ref{configfile}.

These tags have the format:

\begin{verbatim}<trajectory stride=`' filename=`' format=`' cell_units=`' flush=`' bead=`'>
   traj_name{units}(arg1;...)
</trajectory>\end{verbatim}This is very similar to the {}``properties'' tag, but it has the
additional tags {}``format'' and {}``cell\_units'', and only one
trajectory can be specified per file. `format' specifies the format
of the output file, and `cell\_units' specifies the units in which
the cell dimensions are output. Depending on the chosen trajectory
to output, it will either print a file per bead or per atom. If the
trajectory is output per bead then the output files will be {}``filename''
with the bead index appended, so as to distinguish between the trajectories
of each bead. In this case it is also possible to only output one
trajectory by specifying the {}``bead'' attribute. 

The possible choices of output trajectories are:

\input{input_docs/trajectory_list}


\subsection{Checkpoint files}

\label{checkpoint}

As well as the above output files, the state of the system at a particular
time step can also be saved to file. These checkpoint files can be
used as valid input files, with all the information required to restore
the state of the system to the point at which the file was created. 

The syntax for this tag is:

\begin{verbatim}<checkpoint stride=`' filename=`' overwrite=`'>
   step
</checkpoint>\end{verbatim}

Again, this is similar to the {}``trajectory'' and {}``properties''
tags, but instead of having a value which specifies what to output,
the value simply gives a number to identify the current checkpoint
file. There is also one additional attribute, {}``overwrite'', which
specifies whether each new checkpoint file overwrites the old one,
or whether all checkpoint files are kept. If they are kept, they will
be written not to the file {}``filename'', but instead an index
based on the value of {}``step'' will be appended to it to distinguish
between different files.

If the `step' parameter is not specified, the following syntax can
also be used:

\begin{verbatim}<checkpoint stride=`' filename=`' overwrite=`'/>\end{verbatim}


\subsubsection{Soft exit and RESTART}

As well as checkpoint files during a simulation run, \ipi{} also
creates a checkpoint automatically at the end of the simulation, with
file name {}``RESTART''. In the same way as t

he checkpoint files generated above it contains the state of the system
as created by the wrapper, but it doesn't need to be asked for by
the user in the input file. Its purpose is that if the user decides
that insufficient steps were used in an already completed simulation,
then this file can be used to continue from where it ended.

To stop the program in such a way that it makes sure to save the data
generated in a restart file, simply create a file {}``EXIT'' in
the directory in which the code is running. The thread handler will
automatically detect this and safely shut down the program, outputting
the restart file as detailed above. 

An important point to note is that since each time step is split into
several parts, it is only at the beginning of each step that all the
variables are consistent with each other in such a way that the simulation
can be restarted from them without changing the dynamics. Thus if
a soft exit call is made during a step, then the restart file that
is created must correspond to the state of the system at the start
of the step. To this end, the state of the system is saved at the
start of every step.


\section{Distributed execution}

\label{distrib}


\subsection{Sockets}

\ipi only creates a server, it must interface with a client code
in order to run any simulations. The overarching design principle
of \ipi is that these two codes should be as independent as possible,
and so the only communication between them is done through a socket.
A socket is a data transfer device that is designed for internet communication,
so supports both multiple client connections to the same server and
two way communication. This makes sockets ideal for use in \ipi,
where each calculation may require multiple client codes. 

Sockets are described by an IP address and a port number, and can
either be an internet socket, capable of inter-computer communication,
or a unix socket, which is optimized for local communication. The
IP address is input by the user in the {}``address'' tag, and can
be specified in two ways. Firstly, every network has a unique numeric
code of the form 123.45.678.901. Secondly, most networks are named,
so the name of the network on which the server code is running can
be used as an alias for the IP address. A specific case of this is
{}``localhost'', which is the generic name for the local network
used by unix sockets.

The port number is an integer between 1 and 65535 used to distinguish
between all the different sockets open on a particular computer. As
many of the lower numbers are protected for use in important system
processes or internet communication, it is generally advisable to
only use numbers in the range 1025-65535 for simulations.

There are two modes of socket, internet and unix. Unix sockets are
optimized for local communication, whereas internet sockets can be
used to connect to one computer from another. While they are faster
than internet sockets, unix sockets should only be used if a simple
empirical potential is being run, since they are far less flexible.
Also, if complicated empirical potentials or \emph{ab initio} potentials
are used the force calculation becomes the bottleneck in the code,
so the advantage of using unix sockets is lost. 

Internet sockets have two main advantages that make them much more
useful in general. Firstly, the client code does not need to be run
on the same computer as the server code. In particular, if the client
codes are being run on a cluster this means that you do not need to
waste computing resources running the server, which is idle most of
the time. Secondly, the port number is not used for unix sockets,
so if more than one calculation is being run concurrently internet
sockets must be used, so that the client codes connect to the correct
server code.

There are a two other input parameters that may be used to specify
how the server socket looks for client codes to connect to. {}``latency''
specifies the length of time between each check to see if any new
client codes have connected, and {}``slots'' specifies how many
client codes can queue between checks. Neither is likely to be important
in getting the code to run, but may be used to optimize the connection
time if required.


\subsubsection{Data transfer}

Once at least one client code has connected to the server socket,
the force calculation can start. For this to happen the client code
needs the system configuration, and likewise the server needs the
force and potential data to be returned so that it can continue propagating
the dynamics.

To make sure that the connection is good, a simple query-response
data transfer protocol is used. Before any data is sent through the
socket a header string of 12 characters is sent to verify which stage
of the calculation the server is at. Once the client sends the appropriate
response header string, the data is transferred. If no response is
given, the server will either wait and try again, or disconnect the
client code and reassign its job to another, depending on the severity
of the problem. The server assumes that 4-bit integers, 8-bit floats
and 1-bit characters are used. A typical step is of the form:
\begin{enumerate}
\item A header string of {}``\textbf{STATUS}'' is sent by the server socket.
\item A header string is then returned, giving the status of the client
code. Recognized options are:

\begin{description}
\item [{{}``NEEDINIT'':}] If the client code needs any initialising data,
it can be sent here. The server code will then send a header string
{}``INIT'', followed by an integer giving the number of bits in
the initialization string, then the initialization string itself.
\item [{{}``READY'':}] Sent if the client code is ready to calculate
the forces. The server socket will then send a string {}``POSDATA'',
then nine floats the cell vector matrix, then another nine floats
for the inverse matrix, (which need to be transposed if the client
code is written in fortran). The server socket will then send one
integer giving the number of atoms, then the position data as 3 floats
for each atom giving the 3 cartesian components of the bead position.
\item [{{}``HAVEDATA'':}] This is sent if the client has calculated the
potential and forces. The server socket then sends a string {}``GETFORCE'',
and the client socket returns {}``FORCEREADY''. The potential is
then returned as a float, the number of atoms as an integer, then
the force data as 3 floats per atom in the same way as the positions,
and the virial as 9 floats in the same way as the cell vector matrix.
\end{description}
\item The server socket waits until the force data for each replica of the
system has been calculated and returned, upon which the job is finished
and the molecular dynamics loop starts.
\end{enumerate}

\subsubsection{Parallelization}

As mentioned before, one of the primary advantages of using this type
of data transfer is that it allows multiple client codes to connect
to one server code, so that different replicas of the system can be
assigned to different client codes and their forces computed in parallel.
In fact it is trivially parallel, in the sense that no communication
between the client codes is necessary.

In the case that there are as many client codes as replicas of the
system this is very simple, as each replica will be assigned its own
client code. In subsequent steps, the interface will attempt to assign
a particular replica of the system to the client code which calculated
the forces for it the last step. This reduces the change in the particle
positions between calculations for a particular client code, so the
next step is done more efficiently. In the case where there are fewer
client codes than beads, the socket interface will assign spare jobs
to the first client code that finishes running, after matching all
possible jobs to the same client code that calculated it last time
step.

This flexibility is especially useful when the calculations are being
run on a cluster, as in this case the client codes will connect whenever
they reach the front of the queue. More client codes can connect at
any time, and if there is a problem and any client code dies it is
simply disconnected from the server socket and any job it was running
reassigned.

Finally, note that many client codes can be parallelized themselves,
using MPI or other similar protocols. This is fully compatible with
\ipi, as it does not matter how the client does the calculation since
only the forces, potential and virial are sent to the server. Information
on how to run MPI processes can usually be found on the website of
the code provider if the client can be parallelized in this way.


\subsection{ssh tunnelling}

\label{ssh_sockets}

One problem that can often crop up when trying to run a \ipi calculation
is that there is a firewall around the network, especially when the
server code is being run on a cluster. This will often result in error
messages such as {}``Error connecting: Connection timed out'' or
{}``Error connecting: Connection refused''.

Let us suppose that the server code is running on a host network server.net,
and that you are using port 12345. However, when you try to connect
to this port from a computer, let us call this client.net, then you
get error messages like the ones above, due to a firewall on server.net
blocking port 12345.

However, assuming that you can create an ssh connection between the
two computers, then you can use this to forward one of the ports on
client.net to connect to the necessary port on server.net. This can
be done with the following shell command:

\begin{verbatim}
> ssh -f -N server.net -L 23451:server.net:12345
\end{verbatim}

The flags -f and -N just put the ssh process in the background. Then
comes the ssh server, which in this case is the computer on which
the server code is running. The -L flag sets up the ssh tunnel itself.
The above code will forward the port 23451 on client.net to the port
12345 on server.net, as required.

Once this is done, then any data sent to port on 23451 on client.net
will be forwarded through the ssh tunnel to port 12345 on server.net.
By connecting to port 23451 on client.net (or localhost, an alias
for the host the code is running on) with the client the code should
now run.

If the ssh connection fails, with an error message {}``ssh: connect
to host server.net port 22: Connection timed out'', then the server.net
server has also been set up with a firewall around the standard ssh
port, port 22. In this case you can change which port is used to one
that is open using the -p flag, for example:

\begin{verbatim}
> ssh -f -N server.net -L 23451:server.net:12345 -p 99
\end{verbatim}


\subsubsection{Using a script to create a ssh connection}

Now let us suppose that client.net is a cluster, and we wish to submit
the client jobs to the queue. The above code as it is will not work,
since ssh requires a password. However, you can set up ssh such that
no password is needed. 

Firstly, on client.net, we need to set up an ssh key. This can be
done using the command:

\begin{verbatim}
> ssh-keygen -t rsa
\end{verbatim}

It will then prompt you for a passphrase twice. Since we wish to have
use this in a job script where we will not be able to enter a password,
just hit enter twice. Note that this will mean that someone with temporary
access to your account could feasibly take a copy of the ssh key and
then be able to use it, so this should be used with caution.

This should now have created two files in the directory \textasciitilde{}/.ssh,
id\_rsa and id\_rsa.pub. These should be readable only by you, so
use the following code to set up the correct file permissions:

\begin{verbatim}
> chmod 600 ~/.ssh/id_rsa ~/.ssh/id_rsa.pub
\end{verbatim}

Finally, copy the contents of the file id\_rsa.pub and append them
to the file authorized\_keys in the directory \textasciitilde{}/.ssh
of server.net. It should now be possible to ssh from client.net to
server.net without using a password. We can now run the ssh command
from a script, and so we can set up a ssh tunnel from a cluster node.
Note that the cluster nodes will have a different IP address to the
head node, so use {}``localhost'' rather than {}``client.net''
in the input file of the client code.


\chapter{Input reference}

\label{hierarchy}

The following chapter gives a complete list of the tags that can be
specified in the xml input file, along with the hierarchy of objects.
Note that every xml input file must start with the root tag {}``simulation''. 

See the accompanying {}``help.xml'' file in the {}``doc'' directory
to see the recommended input file structure.

\input{input_docs/simulation}
\input{input_docs/initializer}
\input{input_docs/init_file}
\input{input_docs/init_pos}
\input{input_docs/init_mom}
\input{input_docs/init_vel}
\input{input_docs/init_lab}
\input{input_docs/init_mass}
\input{input_docs/init_cell}
\input{input_docs/init_therm}
\input{input_docs/ensembles}
\input{input_docs/forces}
\input{input_docs/socket}
\input{input_docs/cell}
\input{input_docs/beads}
\input{input_docs/atoms}
\input{input_docs/normal_modes}
\input{input_docs/barostats}
\input{input_docs/thermostats}
\input{input_docs/prng}
\input{input_docs/output}
\input{input_docs/checkpoint}
\input{input_docs/properties}
\input{input_docs/trajectory}


\chapter{Troubleshooting}

\label{trouble}


\section{Input errors}
\begin{itemize}
\item \emph{not well-formed (invalid token)}: Seen if the input file does
not have the correct xml syntax. Should be accompanied by a line number
giving the point in the file where the syntax is incorrect.
\item \emph{mismatched tag}: One of the closing tags does not have the
same name as the corresponding opening tag. Could be caused either
by a misspelling of one of the tags, or by having the closing tag
in the wrong place. This last one is a standard part of the xml syntax,
if the opening tag of one item is after the opening tag of a second,
then its closing tag should be before the closing tag of the second.
Should be accompanied by a line number giving the position of the
closing tag.
\item \emph{Uninitialized value of type \_\_\_} or \emph{Attribute/Field name \_\_\_ is mandatory and was not found in the input for property \_\_\_}:
The xml file is missing a mandatory tag, i.e. one without which the
simulation cannot be initialized. Find which tag name is missing and
add it.
\item \emph{Attribute/tag name \_\_\_ is not a recognized property of \_\_\_ objects}:
The first tag should not be found within the second set of tags. Check
that the first tag is spelt correctly, and that it has been put in
the right place.
\item \emph{\_\_\_ is not a valid option (\_\_\_)}: This attribute/tag
only allows a certain range of inputs. Pick one of the items from
the list given.
\item \emph{\_\_\_ is an undefined unit for kind \_\_\_} or \emph{\_\_\_ is not a valid unit prefix} or \emph{Unit \_\_\_ is not structured with a prefix+base syntax}:
The unit input by the user is not correct. Make sure it corresponds
to the correct dimensionality, and is spelt correctly.
\item \emph{Invalid literal for int() with base 10: \_\_\_} or \emph{Invalid literal for float(): \_\_\_} or \emph{\_\_\_ does not represent a bool value}:
The data input by the user does not have the correct data type. See
section \ref{ifilestructure} for what constitutes a valid integer/float/boolean
value.
\item \emph{Error in list syntax: could not locate delimiters}: The array
input data did not have the required braces. For a normal array use
{[}{]}, for a dictionary use \{\}, and for a tuple use ().
\item \emph{The number of atom records does not match the header of xyz file}:
Self-explanatory.
\item \emph{list index out of range}: This will normally occur if the configuration
is initialized from an invalid input file. This will either cause
the code to try to read part of the input file that does not exist,
or to set the number of beads to zero which causes this error in a
different place. Check that the input file has the correct syntax.
\end{itemize}

\section{Initialization errors}
\begin{itemize}
\item \emph{Negative \_\_\_ parameter specified.}: Self-explanatory.
\item \emph{If you are initializing cell from cell side lengths you must pass the 'cell' tag an array of 3 floats}:
If you are attempting to initialize a cell using the {}``abc'' mode,
the code expects three floats corresponding to the three side lengths. 
\item \emph{If you are initializing cell from cell side lengths and angles you must pass the 'cell' tag an array of 6 floats}:
If you are attempting to initialize a cell using the {}``abcABC''
mode, the code expects six floats corresponding to the three side
lengths, followed by the three angles in degrees.
\item \emph{Cell objects must contain a 3x3 matrix describing the cell vectors.}:
If you are attempting to initialize a cell using the {}``manual''
mode, the code expects nine floats corresponding to the cell vector
matrix side lengths. Note that the values of the lower-diagonal elements
will be set to zero.
\item \emph{Array shape mismatch in q/p/m/names in beads input}: The size
of the array in question does not have the correct number of elements
given the number of atoms and the number of beads used in the rest
of the input. If the number of beads is nbeads and the number of atoms
natoms, then q and p should have shape (nbeads, 3{*}natoms) and m
and names should have shape (natoms,).
\item \emph{No thermostat/barostat tag provided for NVT/NPT simulation}:
Some ensembles can only be sampled if a thermostat and/or barostat
have been defined, and so for simulations at constant temperature
and/or pressure these tags are mandatory. If you wish to not use a
thermostat/barostat, but still want to keep the ensemble the same,
then use {}``dummy'' mode thermostat/barostat, which simply does
nothing.
\item \emph{Pressure/Temperature should be supplied for constant pressure/temperature simulation}:
Since in this case the ensemble is defined by these parameters, these
must be input by the user. Add the appropriate tags to the input file.
\item \emph{Manual path mode requires (nbeads-1) frequencies, one for each internal mode of the path.}:
If the {}``mode'' tag of {}``normal\_modes'' is set to {}``manual'',
it will expect an array of frequencies, one for each of the internal
normal modes of the ring polymers. 
\item \emph{PA-CMD mode requires the target frequency of all the internal modes.}:
If the {}``mode'' tag of {}``normal\_modes'' is set to {}``pa-cmd'',
it will expect an array of one frequency, to which all the internal
modes of the ring polymers will be set. 
\item \emph{WMAX-CMD mode requires [wmax, wtarget]. The normal modes will be scaled such that the first internal mode is at frequency wtarget and all the normal modes coincide at frequency wmax.}:
If the {}``mode'' tag of {}``normal\_modes'' is set to {}``wmax-cmd'',
it will expect an array of two frequencies, one two set the lowest
frequency normal mode, and one for the other normal mode frequencies.
\item \emph{Number of beads \_\_\_ doesn't match GLE parameter nb= \_\_\_}:
The matrices used to define the generalized Langevin equations of
motion do not have the correct first dimension. If matrices have been
downloaded from \url{http://gle4md.berlios.de/ } make sure that you
have input the correct number of beads.
\item \emph{Initialization tries to match up structures with different atom numbers}:
If in the initialization any of the matrices has an array shape which
do not correspond to the same number of atoms, then they cannot correspond
to the same system. Check the size of the arrays specified if they
have been input manually.
\item \emph{Cannot initialize single atom/bead as atom/bead index \_\_\_ is larger than the number of atoms/beads}:
Self-explanatory. However, note that indices are counted from 0, so
the first replica/atom is defined by an index 0, the second by an
index 1, and so on.
\item \emph{Cannot initialize the momenta/masses/labels/single atoms before the size of the system is known.}:
In the code, a beads object is created to hold all the information
related to the configuration of the system. However, until a position
vector has been defined, this object is not created. Therefore, whichever
arrays are being initialized individually, the position vector must
always be initialized first.
\item \emph{Trying to resample velocities before having masses.}: A Maxwell-Boltzmann
distribution is partly defined by the atom velocity, and so the masses
must be defined before the velocities can be resampled from this distribution.
\item \emph{Cannot thermalize a single bead}: It does not make sense to
initialize the momenta of only one of the beads, and so \ipi does
not give this functionality.
\item \emph{Initializer could not initialize \_\_\_}: A property of the
system that is mandatory to properly run the simulation has not been
initialized in either the {}``initialize'' section or the appropriate
section in beads.
\item \emph{Ensemble does not have a thermostat to initialize} or \emph{There is nothing to initialize in non-GLE thermostats} or \emph{Checkpoint file does not contain usable thermostat data}:
These are raised if the user has tried to initialize the matrices
for the GLE thermostats with a checkpoint file that either does not
have a GLE thermostat or does not have a thermostat at all.
\item \emph{Size mismatch in thermostat initialization data}: Called if
the shape of the GLE matrices defined in the checkpoint file is different
from those defined in the new simulation. 
\item \emph{Replay can only read from PDB or XYZ files -- or a single frame from a CHK file}:
If the user specifies a replay ensemble, the state of the system must
be defined by either a configuration file or a checkpoint file, and
cannot be specified manually.
\end{itemize}

\section{Output errors}
\begin{itemize}
\item \emph{The stride length for the \_\_\_ file output must be positive.}:
Self-explanatory
\item \emph{\_\_\_ is not a recognized property/output trajectory}: The
string as defined in the {}``properties''/''trajectory'' tag does
not correspond to one of the available trajectories. Make sure that
both the syntax is correct, and that the property has been spelt correctly.
\item \emph{Could not open file \_\_\_ for output}: Raised if there is
a problem opening the file defined by the {}``filename'' attribute.
\item \emph{Selected bead index \_\_\_ does not exist for trajectory \_\_\_}:
You have asked for the trajectory of a bead index greater than the
number of the replicas of the system. Note that indices are counted
from 0, so the first replica is defined by an index 0, the second
by an index 1, and so on.
\item \emph{Incorrect format in unit specification \_\_\_}: Usually raised
if one of the curly braces has been neglected.
\item \emph{Incorrect format in argument list \_\_\_}: This will be raised
either if one of the brackets has been neglected, or if the delimiters
between arguments, in this case {}``;'', are not correct. This is
usually raised if, instead of separating the arguments using {}``;'',
they are instead separated by {}``,'', since this causes the property
array to be parsed incorrectly.
\item \emph{\_\_\_ got an unexpected keyword argument \_\_\_}: This will
occur if one of the argument lists of one of the properties specified
by the user has a keyword argument that does not match any of those
in the function to calculate it. Check the properties.py module to
see which property this function is calculating, and what the correct
keyword arguments are. Then check the {}``properties'' tag, and
find which of the arguments has been misspelt. 
\item \emph{Must specify the index of atom\_vec property}: Any property
which prints out a vector corresponding to one atom needs the index
of that atom, as no default is specified.
\item \emph{Cannot output \_\_\_ as atom/bead index \_\_\_ is larger than the number of atoms/beads}:
Self-explanatory. However, note that indices are counted from 0, so
the first replica/atom is defined by an index 0, the second by an
index 1, and so on.
\item \emph{Couldn't find an atom that matched the argument of \_\_\_}:
For certain properties, you can specify an atom index or label, so
that the property is averaged only over the atoms that match it. If
however no atom labels match the argument given, then the average
will be undefined. Note that for properties which are cumulatively
counted rather than averaged, this error is not raised, and if no
atom matches the label given 0 will be returned.
\end{itemize}

\section{Socket errors}
\begin{itemize}
\item \emph{Address already in use}: This is called if the server socket
is already being used by the host network. There are several possible
reasons for getting this error. Firstly, it might simply be that two
simulations are running concurrently using the same host and port
number. In this case simply change the port number of one of the simulations.
Secondly, you can get this error if you try to rerun a simulation
that previously threw an exception, since it takes a minute or so
before the host will disconnect the server socket if it is not shut
down cleanly. In this case, simply wait for it to disconnect, and
try again. Finally, you will get this error if you try to use a restricted
port number (i.e. below 1024) while not root. You should always use
a non-restricted port number for \ipi simulations.
\item \emph{Error opening unix socket. Check if a file /tmp/ipi\_\_\_ exists, and remove it if unused.}:
Similar to the above error, but given if you are using a unix socket
rather than an internet socket. Since this binds locally the socket
can be removed by the user, which means that it is not necessary to
wait for the computer to automatically disconnect an unused server
socket. 
\item \emph{Port number \_\_\_ out of acceptable range}: The port number
must be between 1 and 65535, and should be greater than 1024. Change
the port number accordingly.
\item \emph{Slot number \_\_\_ out of acceptable range}: The slot number
must be between 1 and 5. Change the slot number accordingly.
\item \emph{'NoneType' object has no attribute 'Up'}: This is called if
an exception is raised during writing the data to output, and so the
thread that deals with the socket is terminated uncleanly. Check the
stack trace for the original exception, since this will be the actual
source of the problem. Also note that, since the socket thread was
not cleaned up correctly, the server socket may not have been disconnected
properly and you may have to wait for a minute before you can restart
a simulation using the same host and port number.
\end{itemize}

\section{Mathematical errors}
\begin{itemize}
\item \emph{math domain error}: If the cell parameters are defined using
the side lengths and angles, with either a pdb file or using the {}``abcABC''
initialization mode, then for some value of the angles it is impossible
to construct a valid cell vector matrix. This will cause the code
to attempt to take the square root of a negative number, which gives
this exception.
\item \emph{overflow encountered in exp}: Sometimes occurs in NPT runs
when the simulation box {}``explodes''. Make sure you have properly
equilibrated the system before starting and that the timestep is short
enough to not introduce very large integration errors.
\end{itemize}
\bibliographystyle{elsarticle-num-names}
\bibliography{mybib}



\part{CURRENT MANUAL}

\begin{comment}
This manual will be structured as follows: 
\begin{itemize}
\item In chapter \ref{intro} we discuss some of the background information
for the wrapper and define the terms to be used throughout the manual. 
\item In chapter \ref{commpro} we will discuss how the communication between
the driver and wrapper codes are implemented. 
\item In chapter \ref{install} we will discuss how to install the code
and test that it is working.
\item In chapter \ref{run} we explain the procedure for running simulations,
and the form of the input and output files. 
\item In chapter \ref{overview} we explain some of the features of the
wrapper code.
\item In chapter \ref{hierarchy} a full list of the major classes used
in the code is given, along with the appropriate tag names and a brief
description of all the fields that can be specified in the xml input
file.\end{itemize}
\end{comment}
\begin{comment}
\begin{itemize}
\item In chapter \ref{dev} we discuss what is needed to modify the code. \end{itemize}
\end{comment}



\section{Shell variables}

In all that follows, we will make use of the following definitions
of shell variables:

\begin{tabular}{|l|p{10cm}|}
\hline
Name & Definition \\ \hline
src\_dir & The directory in which the wrapper code has been installed \\ \hline
np\_dir & The directory in which the NumPy code has been installed \\ \hline
np\_dd\_dir & The directory in which the NumPy tar file has been downloaded \\ \hline
driver\_dir & The directory in which the driver code has been downloaded \\ \hline
np\_vers & The version of NumPy installed \\ \hline
py\_vers & The version of Python installed \\ \hline
esp\_dd\_dir & The directory in which the Quantum Espresso code has been downloaded \\ \hline
esp\_vers & The version of Quantum Espresso installed \\
\hline
\end{tabular}


\section{Driver compatibility}

There are currently patches available for Quantum Espresso version
4.3.2 and CP2K version 2.2. The patch for Quantum Espresso also works
with version 5.0 with minor modifications.

These codes do not come as part of the \ipi package, and must be
downloaded separately. See chapter~\ref{install} for more details
of how to do this. 


\section{Online resources}

For more information about \ipi and to download the source code go
to {[}appropriate ipi website{]}.

Parameters to run Path Integral with Generalized Langevin Equation
thermostat (PI+GLE) calculations can be found on the website:

\url{http://gle4md.berlios.de/ }


\section{Credits}

\ipi has been developed by Michele Ceriotti and Joshua More. 


\section{Citing \ipi}


\section{License}


\chapter{Communication protocol}

%\label{commpro}


\section{Sockets}

\ipi is only a wrapper code, it must interface with a driver code
in order to run any simulations. The overarching design principle
of \ipi is that these two codes should be as independent as possible,
and so the only communication between them is done through a socket.
A socket is a data transfer device that is designed for internet communication,
so supports both multiple client connections to the same server and
two way communication. This makes sockets ideal for use in \ipi,
where each calculation may require multiple driver codes. 

Sockets are described by an IP address and a port number, and can
either be an internet socket, capable of inter-computer communication,
or a unix socket, which is optimized for local communication. The
IP address is input by the user in the {}``address'' tag, and can
be specified in two ways. Firstly, every network has a unique numeric
code of the form 123.45.678.901. Secondly, most networks are named,
so the name of the network on which the wrapper code is running can
be used as an alias for the IP address. A specific case of this is
{}``localhost'', which is the generic name for the local network
used by unix sockets.

The port number is specified in the code by the {}``port'' tag.
It is an integer between 1 and 65535 used to distinguish between all
the different communication processes a particular computer is running.
As many of the lower numbers are protected for use in important system
processes or internet communication, it is generally advisable to
only use numbers in the range 1025-65535 for simulations.

A unix or internet socket is specified by the {}``mode'' attribute
of the {}``socket'' tag. While they are faster than internet sockets,
unix sockets should only be used if a simple empirical potential is
being run. If complicated empirical potentials or \emph{ab initio}
potentials are used the force calculation becomes the bottleneck in
the code, so the advantage of using unix sockets is lost. 

Internet sockets have two main advantages that make them much more
useful in general. Firstly, the driver code does not need to be run
on the same computer as the wrapper code. In particular, if the driver
codes are being run on a cluster this means that you do not need to
waste computing resources running the wrapper, which is idle most
of the time. Secondly, the port number is not used for unix sockets,
so if more than one calculation is being run concurrently internet
sockets must be used, so that the driver codes connect to the correct
wrapper code.

There are a two other input parameters that may be used to specify
how the server socket looks for driver codes to connect to. {}``latency''
specifies the length of time between each check to see if any new
driver codes have connected, and {}``slots'' specifies how many
driver codes can queue between checks. Neither is likely to be important
in getting the code to run, but may be used to optimize the connection
time if required.


\section{Data transfer}

Once at least one driver code has connected to the server socket,
the force calculation can start. For this to happen the driver code
needs the system configuration, and likewise the wrapper needs the
force and potential data to be returned so that it can continue propagating
the dynamics.

To make sure that the connection is good, a simple query-response
data transfer protocol is used. Before any data is sent through the
socket a header string of 12 characters is sent to verify which stage
of the calculation the wrapper is at. Once the driver sends the appropriate
response header string, the data is transferred. If no response is
given, the wrapper will either wait and try again, or disconnect the
driver code and reassign its job to another, depending on the severity
of the problem. The wrapper assumes that 4-bit integers, 8-bit floats
and 1-bit characters are used. A typical step is of the form:
\begin{enumerate}
\item A header string of {}``\textbf{STATUS}'' is sent by the server socket.
\item A header string is then returned, giving the status of the driver
code. Recognized options are:

\begin{description}
\item [{{}``NEEDINIT'':}] If the driver code needs any initialising data,
it can be sent here. The server code will then send a header string
{}``INIT'', followed by an integer giving the number of bits in
the initialization string, then the initialization string itself.
\item [{{}``READY'':}] Sent if the driver code is ready to calculate
the forces. The server socket will then send a string {}``POSDATA'',
then nine floats the cell vector matrix, then another nine floats
for the inverse matrix, (which need to be transposed if the driver
code is written in fortran). The server socket will then send one
integer giving the number of atoms, then the position data as 3 floats
for each atom giving the 3 cartesian components of the bead position.
\item [{{}``HAVEDATA'':}] This is sent if the driver has calculated the
potential and forces. The server socket then sends a string {}``GETFORCE'',
and the client socket returns {}``FORCEREADY''. The potential is
then returned as a float, the number of atoms as an integer, then
the force data as 3 floats per atom in the same way as the positions,
and the virial as 9 floats in the same way as the cell vector matrix.
\end{description}
\item The server socket waits until the force data for each replica of the
system has been calculated and returned, upon which the job is finished
and the molecular dynamics loop starts.
\end{enumerate}

\section{Parallelization}

As mentioned before, one of the primary advantages of using this type
of data transfer is that it allows multiple driver codes to connect
to one wrapper code, so that different replicas of the system can
be assigned to different driver codes and their forces computed in
parallel. In fact it is trivially parallel, in the sense that no communication
between the driver codes is necessary.

In the case that there are as many driver codes as replicas of the
system this is very simple, as each replica will be assigned its own
driver code. In subsequent steps, the interface will attempt to assign
a particular replica of the system to the driver code which calculated
the forces for it the last step. This reduces the change in the particle
positions between calculations for a particular driver code, so the
next step is done more efficiently. In the case where there are fewer
driver codes than beads, the socket interface will assign spare jobs
to the first driver code that finishes running, after matching all
possible jobs to the same driver code that calculated it last time
step.

This flexibility is especially useful when the calculations are being
run on a cluster, as in this case the driver codes will connect whenever
they reach the front of the queue. More driver codes can connect at
any time, and if there is a problem and any driver code dies it is
simply disconnected from the server socket and any job it was running
reassigned.

Finally, note that many driver codes can be parallelized themselves,
using MPI or other similar protocols. This is fully compatible with
\ipi, as it does not matter how the driver does the calculation since
only the forces, potential and virial are sent to the wrapper. Information
on how to run MPI processes can usually be found on the website of
the code provider if the driver can be parallelized in this way.


\section{ssh tunnelling}

%\label{ssh_sockets}

One problem that can often crop up when trying to run a \ipi calculation
is that there is a firewall around the network, especially when the
wrapper code is being run on a cluster. This will often result in
error messages such as {}``Error connecting: Connection timed out''
or {}``Error connecting: Connection refused''.

Let us suppose that the wrapper code is running on a host network
wrapper.net, and that you are using port 12345. However, when you
try to connect to this port from a computer, let us call this driver.net,
then you get error messages like the ones above, due to a firewall
on wrapper.net blocking port 12345.

However, assuming that you can create an ssh connection between the
two computers, then you can use this to forward one of the ports on
driver.net to connect to the necessary port on wrapper.net. This can
be done with the following shell command:

\begin{verbatim}
> ssh -f -N wrapper.net -L 23451:wrapper.net:12345
\end{verbatim}

The flags -f and -N just put the ssh process in the background. Then
comes the ssh server, which in this case is the computer on which
the wrapper code is running. The -L flag sets up the ssh tunnel itself.
The above code will forward the port 23451 on driver.net to the port
12345 on wrapper.net, as required.

Once this is done, then any data sent to port on 23451 on driver.net
will be forwarded through the ssh tunnel to port 12345 on wrapper.net.
By connecting to port 23451 on driver.net (or localhost, an alias
for the host the code is running on) with the driver the code should
now run.

If the ssh connection fails, with an error message {}``ssh: connect
to host wrapper.net port 22: Connection timed out'', then the wrapper.net
server has also been set up with a firewall around the standard ssh
port, port 22. In this case you can change which port is used to one
that is open using the -p flag, for example:

\begin{verbatim}
> ssh -f -N wrapper.net -L 23451:wrapper.net:12345 -p 99
\end{verbatim}


\subsection{Using a script to create a ssh connection}

Now let us suppose that driver.net is a cluster, and we wish to submit
the driver jobs to the queue. The above code as it is will not work,
since ssh requires a password. However, you can set up ssh such that
no password is needed. 

Firstly, on driver.net, we need to set up an ssh key. This can be
done using the command:

\begin{verbatim}
> ssh-keygen -t rsa
\end{verbatim}

It will then prompt you for a passphrase twice. Since we wish to have
use this in a job script where we will not be able to enter a password,
just hit enter twice. Note that this will mean that someone with temporary
access to your account could feasibly take a copy of the ssh key and
then be able to use it, so this should be used with caution.

This should now have created two files in the directory \textasciitilde{}/.ssh,
id\_rsa and id\_rsa.pub. These should be readable only by you, so
use the following code to set up the correct file permissions:

\begin{verbatim}
> chmod 600 ~/.ssh/id_rsa ~/.ssh/id_rsa.pub
\end{verbatim}

Finally, copy the contents of the file id\_rsa.pub and append them
to the file authorized\_keys in the directory \textasciitilde{}/.ssh
of wrapper.net. It should now be possible to ssh from driver.net to
wrapper.net without using a password. We can now run the ssh command
from a script, and so we can set up a ssh tunnel from a cluster node.
Note that the cluster nodes will have a different IP address to the
head node, so use {}``localhost'' rather than {}``driver.net''
in the input file of the driver code.


\chapter{Installing \ipi}

%\label{install}

\begin{comment}

\section{Background and nomenclature}


\subsection{Molecular dynamics}

Molecular dynamics is a way of sampling the possible states of a system
by propagating the positions and momenta of the particles in the system
in discrete time steps according to a set of physical rules.

By choosing the algorithm used to propagate the momenta correctly,
the probability of the system being in a particular state will be
the same as that of a physically realistic system, such as a system
in contact with an external heat bath.

We will call the complete set of possible states of the system an
ensemble of states. The relevant ensembles we will be interested in
are: 
\begin{description}
\item [{NVE}] ensemble This gives constant particle number, volume and
energy, and corresponds to an isolated physical system. 
\item [{NVT}] ensemble This gives constant particle number, volume and
temperature, and corresponds to a physical system in thermal equilibrium
with a heat bath. 
\item [{NPT}] ensemble This gives constant particle number, pressure and
temperature, and corresponds to a physical system in thermal equilibrium
with a heat bath and in equilibrium with a pressure bath. 
\end{description}
We will call the total number of possible states of a system for any
given set of external variables the partition function of that system.

In any practical simulation, the number of atoms has to be far fewer
than the typical number present in the physical system of interest,
for reasons of computational difficulty. This means that a high proportion
of the particles in the simulation are likely to be on the edge of
the simulation box. To prevent this having a large effect on the properties
of the system, all of the driver codes implement what is known as
periodic boundary conditions. This{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}{*}88 


\subsection{Path integral generalization}

In the path integral generalization of classical mechanics, we use
the following isomorphism between the quantum partition function of
a system and a classical partition function in a higher phase space:
\begin{equation}
Q=\Tr(e^{-\hat{H}\beta})
\end{equation}
 
\begin{align}
 & Q=\lim_{N\to\infty}\prod_{i=1}^{N}\left[\int_{-\infty}^{\infty}\left(\frac{2\pi m}{h^{2}\betan}\right)^{\frac{3}{2}}e^{-\hat{U}_{N}\betan}\dd\textbf{r}_{i}\right]\\
 & \hat{U}_{N}=\sum_{i=1}^{N}\left[\frac{m}{2\betan^{2}\hbar^{2}}(\textbf{r}_{i}-\textbf{r}_{i-1})^{2}+\hat{V}_{i}\right]\\
 & \betan=\frac{\beta}{N}\nonumber 
\end{align}
 Where \ensuremath{\hat{H}}
 is the quantum Hamiltonian of the system, which is the operator corresponding
to the observable energy, \ensuremath{\hat{V}_{i}}
 is the potential of the replica of the system corresponding to the
label \ensuremath{i}
, and \ensuremath{\hat{U}_{N}}
 is a classical configurational Hamiltonian in the extended phase
space, which only depends on particle coordinates.

In practice we do not need to go to the limit of infinite \ensuremath{N}
, as convergence can often be achieved with a small increase of phase
space. Note that this classical Hamiltonian is equivalent to that
of a \ensuremath{N}
 replicas of the original system, with a harmonic potential between
adjacent replicas. We will call this set of replicas of each atom
a ring polymer, and the component particles of this ring polymer beads.

The last thing we must do to make the connection with molecular dynamics
is to add fictitious ring polymer momenta to allow us to sample the
phase space more effectively. In our code we make the choice corresponding
to assigning each bead the mass of the corresponding atom: 
\begin{align}
 & Q=\lim_{N\to\infty}\prod_{i=1}^{N}\left[\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\left(\frac{1}{2\pi\hbar}\right)^{3}e^{-\hat{H}_{N}\betan}\dd\textbf{r}_{i}\dd\textbf{p}_{i}\right]\\
 & \hat{H}_{N}=\hat{U}_{N}+\sum_{i=1}^{N}\frac{\textbf{p}_{i}^{2}}{2m}
\end{align}


This is then the partition function corresponding to the NVE ensemble
for the quantum system, and as it has a purely classical Hamiltonian
the algorithms used in classical dynamics and the generalizations
used to sample other ensembles can be applied to this system in exactly
the same way as for a classical system. 


\subsection{Ab initio potentials}

An ab initio potential is one that does not require any experimentally
derived parameters in its calculation. Under the strictest sense of
the term many of the most popular DFT algorithms are not ab initio
as they do use experimentally determined parameters, but as prior
simulations are not necessary for the formulation of a DFT potential,
we will define DFT as such for the purposes of this manual.

DFT calculations are done by minimising an energy with respect to
the one electron probability density, rather than the wavefunction
as is common in most other ab initio potentials. The energy is given
in terms of that of a fictitious system without electron interaction,
for which the energy can be found exactly, plus an energy term due
to electron correlation. This is the quantity of fundamental importance
in DFT calculations, and while no analytic form for it exists several
useful approximations to it have been created. We will call these
correlation functionals.

DFT calculations are done via an iterative procedure where a guess
for the one electron wavefunctions are made so that the energy can
be calculated, and a new set of one electron wavefunctions are then
calculated by minimising this energy. The new wavefunctions are then
put back into the first step and the procedure repeated until the
initial and final density matrices are consistent.

In what is known as Car-Parrinello molecular dynamics, the converged
wavefunctions of one step are then used as the starting point for
the next step, drastically accelerating the convergence.

Another important consideration is the basis set used for the 
\end{comment}



\section{System requirements}

To install and run \ipi, you will need to have:
\begin{itemize}
\item A modern C compiler (for the sockets.c module)
\item Python version 2.4 or greater
\item The Python numerical library NumPy
\end{itemize}
Note that \ipi does not need to be compiled, it will run automatically
as a script if the appropriate libraries have been installed.

Additionally, most driver codes will have their own requirements.
Many of them, including the test driver codes given in the {}``forces''
directory, will need a modern Fortran compiler. Most will also need
to be linked with some mathematical libraries, such as BLAS, FFTW
and LAPACK. Installation instructions should be provided as part of
the code distribution and on the appropriate website.


\section{\ipi download}

A tar file can be downloaded from the website {[}appropriate wrap-pi
website{]}. To install this you need to input the following commands:

\begin{verbatim}
> cd $src_dir
> tar xf [wrapper_tar_file.tar]
\end{verbatim}

You can also download the git repository at {[}github repository name{]}
using the commands:

\begin{verbatim}
> cd $src_dir
> git clone [github repository name]
\end{verbatim}


\section{NumPy download}

NumPy is the standard Python mathematics library, and is used for
most of the array minipulation and linear algebra in the wrapper code.
How difficult it is to install NumPy depends on whether you have root
access or not, and on the version of Python you are using.

In any case you must first obtain the NumPy code. The NumPy library
can be downloaded as a tar file from \url{http://sourceforge.net/projects/numpy/files/NumPy/}
This must then be extracted using the following commands:

\begin{verbatim}
> cd $np_dd_dir
> tar czf $np_vers.tar.gz
\end{verbatim}

Before installing this code it first needs to be configured correctly.
Note that this requires the distutils package that comes with the
python-dev package. Assuming that the required software is installed,
the NumPy package is built using:

\begin{verbatim}
> python setup.py build
\end{verbatim}

The next step is to install the package. By default the download is
to the directory /usr/local. If you have root access, and so can write
to /usr, then all that needs to be done to finish the install is:

\begin{verbatim}
> python setup.py install
\end{verbatim}

If you do not have root access, then the next step depends on which
version of Python is beind used. With versions 2.6 or later there
is a simple command to automatically download into the directory \$HOME/local:

\begin{verbatim}
> python setup.py install --user
\end{verbatim}

With Python 2.4/2.5 the process is a little more involved. First you
must explicitly install the package in the directory of choice with
the following command:

\begin{verbatim}
> python setup.py install --prefix=$np_dir
\end{verbatim}

Next, you must tell python where to find this library, by appending
to the linux environment variable \textbf{PYTHONPATH}:

\begin{verbatim}
> export PYTHONPATH=$PYTHONPATH:$np_dir/lib64/$py_vers/site-packages
\end{verbatim}

You may have to replace lib64 with lib depending on whether you are
installing to a 64-bit or 32-bit architecture.


\section{PyFFTW}

Currently, the normal mode transformation is, at least by default,
computed using a fast-Fourier transform (FFT) library within the NumPy
distribution. This however is not the only distribution that could
be used, and indeed faster stand-alone versions exist. The gold-standard
FFT library is the FFTW library, which is a set of C libraries that
have been heavily optimized for a wide range of applications. There
have been a number of Python wrappers around the FFTW library, of
which currently one has been interfaced with \ipi. This code can
be found at \url{https://github.com/hgomersall/pyFFTW}, and has documentation
at \url{http://hgomersall.github.io/pyFFTW/}.

This code has the following dependencies:
\begin{itemize}
\item Python version 2.7 or greater
\item Numpy version 1.6 or greater
\item FFTW version 3.2 or greater
\end{itemize}
This can be installed in the same way as NumPy, except using the code
distribution above, or using various installation packages as per
the instructions on the above documentation. Note that no other options
need to be specified in the input file, the wrapper will check to
see if this library can be installed, and if it can it will be used
by default. Otherwise the slower NumPy version will be used.


\section{Driver installation and compilation}


\subsection{Quantum Espresso}

You can download the source code for Quantum Espresso at:

\url{www.quantum-espresso.org/}

For Quantum Espresso version 4.3.2, there is a patch file to allow
it to be used as a driver code with \ipi in the directory \$src\_dir/patches/pwscf.
This can be applied to the Quantum Espresso source code using:

\begin{verbatim}
> cd $esp_dd_dir
> tar zxvf $esp_vers.tar.gz
> cd $esp_vers
> patch -p1 < $src_dir/patches/pwscf/pw-driver.patch
\end{verbatim}

After this, continue the download as per the instructions at:

\url{www.quantum-espress.org/}.


\subsection{CP2K }

%I still don't even have this code, so can't write this bit...


\section{Testing}

%\label{tests}

Several test cases are distributed with the code to ensure that your
distribution is working correctly. There are also simple tests to
see if the driver codes are working correctly.

All the input files are contained in the directory test, which is
subdivided into the following directories:
\begin{description}
\item [{lj:}] This gives a simple classical Lennard-Jones simulation of
Ne. The state points are given by (N, \(\rho\), T) = (864, 0.35,
1.62), (N, \(\rho\), T) = (864, 0.75, 1.069) and (N, \(\rho\), T)
= (864, 0.88, 1.095) in reduced Lennard-Jones units, so that the results
can be compared to those in the paper \cite{lverlet67pr}.
\item [{ph2:}] This simulates para-hydrogen using the isotropic Silvera-Goldman
pair potential. There are three directories, {}``RPMD'', {}``nvt''
and {}``Tuckerman''. {}``RPMD'' and {}``nvt'' have tests which
can be compared to the results of \cite{mill-mano05jcp}, and {}``Tuckerman''
has tests which can be compared to the results of \cite{mart+99jcp}.
\item [{pwscf:}] This has two simple examples to test to see if the Quantum-Espresso
driver is functioning correctly. There is one simple 4-atom lithium
test, and a test using a single water molecule.
\item [{harmonic:}] This has a simple example of a 1D harmonic oscillator.
This demonstrates the displaced path integral momentum distribution
operator as given in \cite{linlin+10prl}. As the momentum distribution
is known analytically for this simple system, this provides an indication
of how well the method is working.
\end{description}

\chapter{Running \ipi}

%\label{run}


\section{How to run the wrapper}

\ipi simulations are run using the top level script found in the
src directory. If the xml-formatted input file is called input\_file.xml,
then the command to run a \ipi simulation is:

\begin{verbatim}
> python $src_dir/i-pi input_file.xml
\end{verbatim}

The code will then have to wait until at least one driver code has
connected before running any dynamics. At this point the code is essentially
idle, and the only process that it runs is to periodically poll the
socket for connections.


\section{How to run the driver}


\subsection{CP2K}

To run CP2K with the wrapper, an additional file named {}``serverfile''
must be kept in the same directory that CP2K is being run from. It
should contain a string a single line long, of the format {}``mode:host:port\_number''.
{}``mode'' should either be {}``INET'' or {}``UNIX'', representing
an internet or unix socket respectively. The rest of the input file
is the same as for a standard CP2K calculation.


\subsection{Quantum-Espresso}

Suppose we want to run Quantum-Espresso as the driver code, and we
have compiled it as given above, and have an input file which will
give the correct forces and potential for the system of interest.
If we want to connect to a socket on the host address host\_address
and on the port number port, then we would add the following lines
to the input file:

\begin{verbatim}
&CONTROL
   ...
   calculation=`driver'
   srvaddress=`host_address:port'
   ...
/
\end{verbatim}

If we wanted to run on a UNIX port instead of an INET port, we would
instead write:

\begin{verbatim}
&CONTROL
   ...
   calculation=`driver'
   srvaddress=`UNIX:host_address:port'
   ...
/
\end{verbatim}

The rest of the input file should be the same as for a standard Quantum
Espresso calculation.


\section{Running simulations}


\subsection{Running a simulation}

There are two parts to any \ipi simulation. First the wrapper is
run, which starts the dynamics loop until the forces are required.
Once the server socket has been opened, it then waits for connections
from driver codes. 

For a small simulation, with a simple classical potential, it may
be preferable to run both halves of the simulation on the same computer,
especially as this allows us to use the faster unix domain sockets.
This can be done using a single script, such as:

\begin{verbatim}
#These are the parameters that need to be specified by
#the user. The number of driver codes is given by $ndrivers,
#the i-pi source directory is given by $src_dir, and the code
#that runs one of the driver codes is given by $driver_comm
inputfile=...
hostname=...
port=...
ndrivers=...
src_dir=...
driver_comm="..."

#This modifies the input file. If necessary, the driver code
#input file should be modified here too
sed -i "s/<address>[^<]*</<address>$hostname</" $inputfile
sed -i "s/<port>[^<]*</<port>$port</" $inputfile

if [ -e EXIT ]; then
   rm EXIT 
fi

#Runs the simulation, and redirects the wrapper
#standard output to a file named 'log'.
bash -c "python $src_dir/i-pi $inputfile > log &"

sleep 20 #must wait for server to be initialized
for a in `seq 1 $ndrivers`; do
   bash -c "$driver_comm"
done 
\end{verbatim}

However, for most simulations, especially those running with expensive
\emph{ab initio} forces and potentials, we will want to run the driver
codes on a computational cluster. Here we run the wrapper code elsewhere,
and use internet sockets for the communication. For information on
how to do this, see section \ref{ssh_sockets}.

To run the driver on a cluster computer, two major changes must be
implemented. Firstly, the code required to run the driver code should
be separated from that of the wrapper code. Secondly, the driver script
should be executed using a queueing program such as qsub rather than
the bash shell script as above, and so will need to have the appropriate
directive comment lines. Depending on which cluster is used, additional
commands to set parameters such as the walltime, number of nodes and
number of cores can be used. Check the qsub man page for the cluster
you are running the code on for more details.

One possible driver script layout would be:

\begin{verbatim}
#!/bin/bash
#$ -S /bin/bash
#$ -N jobname
#$ ...

#use this if there are library files needed
#by the driver code that are not on the default path

#export LD_LIBRARY_PATH=...

#use this if ssh tunnelling is required

#wrapper_host=... #the wrapper host address
#wrapper_port=... #the wrapper port number
#driver_port=... #the cluster port number
#ssh -f -N $wrapper_host -L $driver_port:$wrapper_host:$wrapper_port

inputfile=...
hostname=...
port=...
driver_comm="..."

#Modify driver input file if necessary

bash -c "driver_comm"
\end{verbatim}

Note that it may be necessary to export libraries or modules if they
are not present by default, or to use an ssh tunnel if the cluster
is not open to the internet (see \ref{ssh_sockets}).


\subsection{Running multiple simulations}

To set up multiple concurrent \ipi simulations each host socket has
to be distinguishable, so that the driver codes appropriate to each
simulation connect only to the correct wrapper code. This can be done
with internet sockets by adjusting the port number for each simulation,
which then acts as a unique identifier for each server. The shell
scripts from before can be easily modified to do this, for example
if we assume the simulations will be run in directories called sim\_1,
sim\_2, \ldots, then we can write:

\begin{verbatim}
first_port=... #The value of the port of sim_1
i=0
for a in sim_*; do 
   cd $a
   port=$((first_port+i))
   ((i++))

   #here we run the code for one simulation as before

   cd ..
done
\end{verbatim}


\section{Input files}


\subsection{xml file}

This is the main input file from which the simulation parameters are
initialized. An xml file consists of a set of hierarchially nested
tags. There are three parts to an xml tag. Each tag is identified
by a tag name, which specifies the class or variable that is being
initialized. Between the opening and closing tags there may be some
data, which may or may not contain other tags. This will specify the
contents of a class, or the value of a variable. Finally, any tag
can have attributes, which in the code are used for variables which
specify the state or type of class that will be created. A xml tag
has the following syntax:

\begin{verbatim}
<tag_name attribute_name=`attribute_data'>tag_data</tag_name>
\end{verbatim}

The syntax for the different types of tag data is given below:

\begin{tabular}{|c|c|}
\hline 
Data type & Syntax\tabularnewline
\hline 
\hline 
Boolean & <tag>True</tag> or <tag>False</tag>\tabularnewline
\hline 
Float & <tag>11.111</tag> or <tag>1.1111e+1</tag>\tabularnewline
\hline 
Integer & <tag>12345</tag>\tabularnewline
\hline 
Dictionary & <tag>\{name1: data1, name2: data2, \ldots \}</tag>\tabularnewline
\hline 
String & <tag>string\_data</tag>\tabularnewline
\hline 
Tuple & <tag> (int1, int2, \ldots )</tag>\tabularnewline
\hline 
\end{tabular}

Also, for arrays, which have the `shape' attribute, we have the syntax

\begin{verbatim}
<tag shape=`array_shape'>[entry1, entry2, ... ]</tag>
\end{verbatim}

where each entry has the same data type, which will be either float,
int, boolean or string, and `array\_shape' is a tuple that gives the
shape of the generated array. If the shape is not specified, then
a 1D array will be assumed.

The code uses this hierarchial structure to help read the data; if
a particular object is held within a parent object in the code, then
the tag for that object will be within the appropriate parent tags.
This is used to make the structure of the simulation clear. For example
the system that is being studied is partly defined by the thermodynamic
ensemble that should be sampled, which in turn may be partly defined
by the temperature, and so on. In the input file this would be specified
by having a {}``simulation'' tag, containing an {}``ensemble''
tag, which itself contains a {}``temperature'' tag, which will contain
a float value corresponding to the temperature. In the code itself
this will correspond to a simulation object, which will contain an
ensemble object, which will contain a temperature variable. In this
manner, the simulation class structure can be constructed iteratively.

To help detect any user error the acceptable tag names, type of data
and possible default values are all specified in the code, in a specialized
input class for each class of object. A full list of all the available
tags and a brief description of their function is given in chapter~\ref{hierarchy}.


\subsection{Configuration file}

%\label{configfile}

The initial configuration data can be initialized both through the
xml file, and through a separate configuration file. The configuration
file is specified within the {}``initialize'' tag name in the xml
file, with the {}``mode'' attribute as the file format. The currently
accepted file formats are:
\begin{itemize}
\item pdb
\item xyz
\end{itemize}
These files can be used to specify the initial atom positions, labels
and masses (based on atomic symbol) and, depending on the format,
the cell parameters as well. It can either be held within a {}``file''
tag, in which case all applicable information will be initialized,
or {}``positions''/''masses''/''labels''/''cell'', in which
case only the specified property will be initialized.


\subsection{Units}

%\label{units}

\ipi uses the following set of internal units:

\begin{tabular}{|l|l|l|}
\hline 
Unit & Name & S.I. Value\tabularnewline
\hline 
\hline 
Length & Bohr radius & 5.2917721e-11 m\tabularnewline
\hline 
Time & N.A. & 2.4188843e-17 s\tabularnewline
\hline 
Mass & Electron mass & 9.1093819e-31 kg\tabularnewline
\hline 
Temperature & kelvin & 1 K\tabularnewline
\hline 
Energy & Hartree & 4.3597438e-18 J\tabularnewline
\hline 
Pressure & N.A. & 2.9421912e13 Pa\tabularnewline
\hline 
\end{tabular}

However, these units can be overriden in the input file by using the
{}``units'' attribute. For example, if you wanted to specify the
cell parameters in units of angstrom rather than bohr radii then in
the input file you would specify:

\begin{verbatim}<cell units=`angstrom'> ... </cell>\end{verbatim} 


\section{Output files}

All output files are specified within the tag name {}``output''
in the xml file. Each tag held here specifies a particular output
file, with data specifying the file name, the data to be output and
the number of steps between each write, plus other relevant data.


\subsection{Property file}

%\label{propertyfile}

This is the output file for for all the system and simulation level
properties, such as the total energy and the overall time taken. The
properties that are output are determined by the {}``properties''
tag in the xml input file. The format of this tag is:

\begin{verbatim}<properties stride=`' filename=`' flush=`' shape=`'>
   [ prop1name{units}(arg1; ... ), prop2name{...}(...), ...  ]
</properties>\end{verbatim}

The attributes have the following meanings:
\begin{description}
\item [{stride}] The number of steps between each output to file
\item [{filename}] The name of the output file
\item [{flush}] The number of steps between flushing the buffer
\end{description}
This tag data is an array of strings, each of which containing three
different parts:
\begin{itemize}
\item First, the property name, which describes which type of property is
to be output. This is a mandatory part of the string.
\item Secondly, you can specify the units that the property will be output
in. This is specified between curly brackets, as shown above. If this
is not specified, then the property will be output in atomic units.
\item Thirdly, the arguments to be passed to the function can be specified
between standard brackets, with each argument separated by a semi-colon.
These may or may not be mandatory depending on the property. The arguments
can be specified by either of two different syntaxes, (name1=arg1;
\ldots ) or (arg1; \ldots ). The first explicitly assigns the argument
with the name {}``name1'' the value {}``arg1'', whereas the second
relies on the arguments being specified in the correct order. The
two syntaxes may be mixed, but positional arguments must be specified
first otherwise undefined behaviour will result. If no arguments are
specified, then the defaults as defined in the properties.py module
will be used.
\end{itemize}
The different available property names are:

\input{input_docs/property_list}

The format of the file is given by a comment line per property giving
the name of the properties and which columns of the input file will
be used to print it, followed by their values in the appropriate units
at regular intervals given by the {}``stride'' list of the input
file. The file is fixed formatted, with two blank characters at the
start of each row, then the data in the same order as the header row.
Each column is 16 characters wide and every float is written in exponential
format with 8 digits after the decimal point.


\subsection{Trajectory files}

These are the output file for atomic or bead level properties, such
as the bead positions. Each trajectory that should be output is specified
by the {}``trajectory'' tag in the input file. These tags have the
format:

\begin{verbatim}<trajectory stride=`' filename=`' format=`' cell_units=`' flush=`' bead=`'>
   traj_name{units}(arg1;...)
</trajectory>\end{verbatim}

This is very similar to the {}``properties'' tag, but it has the
additional tags {}``format'' and {}``cell\_units'', and only one
trajectory can be specified. `format' specifies the format of the
output file, and `cell\_units' specifies the units in which the cell
dimensions are output. Depending on the chosen trajectory to output,
it will either print a file per bead or per atom. If the trajectory
is output per bead then the output files will be {}``filename''
with the bead index appended, so as to distinguish between the trajectories
of each bead. In this case it is also possible to only output one
trajectory by specifying the {}``bead'' attribute. The possible
choices of output properties are:

\input{input_docs/trajectory_list}

The allowable file formats for the trajectory output files are the
same as for the configuration input files, given in~\ref{configfile}.


\section{Restart mechanism}


\subsection{Checkpoint files}

As well as the above output files, the state of the system at a particular
time step can also be saved to file. These checkpoint files can be
used as valid input files, with all the information required to restore
the state of the system to the point at which the file was created.
The syntax for this tag is:

\begin{verbatim}<checkpoint stride=`' filename=`' overwrite=`'>
   step
</checkpoint>\end{verbatim}

Again, this is similar to the {}``trajectory'' and {}``properties''
tags, but instead of having a value which specifies what to output,
the value simply gives a number to identify which checkpoint is which.
There is also one additional attribute {}``overwrite'', which specifies
whether each new checkpoint file overwrites the old one, or whether
all checkpoint files are kept. If they are kept, they will be written
not to the file {}``filename'', but instead an index will be appended
to it to distinguish between different files.

If the `step' parameter is not specified, the following syntax can
also be used:

\begin{verbatim}<checkpoint stride=`' filename=`' overwrite=`'/>\end{verbatim}


\subsection{Restart file}

As well as checkpoint files during a simulation run, ipi also creates
a file to restart from automatically at the end of the simulation,
with file name {}``RESTART''. In the same way as the checkpoint
files generated above, it contains the state of the system is created
by the wrapper, but it doesn't need to be asked for by the user in
the input file. Its purpose is that if the user decides that insufficient
steps where used in an already completed simulation, then this file
can be used to continue from where it ended.


\subsection{Soft exit}

To stop the program in such a way that it makes sure to save the data
generated in a restart file, simply create a file {}``EXIT'' in
the directory in which the code is running. The thread handler will
automatically detect this and safely shut down the program, outputting
the restart file as detailed above. 

An important point to note is that since each time step is split into
several parts, it is only at the beginning of each step that all the
variables are consistent with each other in such a way that the simulation
can be restarted from them without changing the dynamics. Thus if
a soft exit call is made during a step, then the restart file that
is created must correspond to the state of the system at the start
of the step. To this end, the state of the system is saved at the
start of every step.


\subsection{Initialization from RESTART}

As well as being a valid input file, the RESTART file can be used
as a starting point for simulations in two other ways. Firstly, in
the {}``initialize'' tag, there is a {}``chk'' option for the
{}``mode'' attribute of all the initializable properties. This will
use the restart file as a basis for initializing the specified bits
of data.

Secondly, there is a particular ensemble type, {}``replay\_file'',
which takes a configuration file with multiple frames or a checkpoint
file with one frame and reruns it without doing dynamics, by simply
setting the configuration to match that given by the input file at
each frame. 


\chapter{Feature overview}

%\label{overview}


\section{Ring polymer contraction}

Often, we will want to take a description of the system in terms of
a certain number of beads and instead represent it with a different
number of beads. In the wrapper this is done using a ring polymer
contraction scheme~\cite{mark-mano08jcp}.

This works by matching the normal mode amplitudes of the old ring
polymer with those of the new ring polymer. The normal modes are matched
in ascending frequency order, until either all the ring polymer normal
modes of the new ring polymer have been assigned a frequency, or all
the original ring polymer normal modes have been used. Any remaining
normal mode amplitudes are set to zero.

This is used in two different cases:
\begin{itemize}
\item Initializing a simulation from a previous one with a different number
of beads.
\item Calculating long range interactions with a reduced number of beads.
\end{itemize}
The first of these points is fairly self-explanatory. To specify the
number of beads that the system should be initialized to, the {}``nbeads''
attribute of the {}``initialize'' tag simply needs to be set to
the appropriate value in the input file. This will take whatever configuration
is specified in the initilialization and use the above transformation
to give a new configuration with the correct number of beads. 

The second of these uses is more complicated. To understand this,
consider an empirical potential model that can be split into a short-ranged
potential and a long-ranged potential, where the long-ranged part
of the potential is slowly varying in space. This means that the long-ranged
part of the potential will require fewer beads to converge the calculated
potential energy than the short-ranged part, as it only varies slightly
over the ring polymer length scale. 

Also, let us suppose that the calculation of the long-range terms
is the computational bottleneck, as they are more numerous than the
short-range terms. In this case calculating the long-range part of
the potential on a contracted ring polymer will have a large effect
on the simulation time, even if the short-range part is calculated
on the full ring polymer. Therefore this technique allows us to speed
up the simulation without sacrificing accuracy.

Once the force, potential and virial have been calculated using the
reduced ring polymer, the resulting force and virial vectors are transformed
back to the appropriate size for the full ring polymer phase space,
before being used to propagate the trajectory.

For a given {}``socket'' tag, this can be specified in the input
file by setting the {}``nbeads'' attribute. This value will dictate
how many beads will be used to calculate that part of the force, and
if it is not equal to the number of beads in the system then ring
polymer contraction will be used.


\section{Constant temperature simulations}

There are a variety of different stochastic thermostats implemented
within the wrapper. All the algorithms that are used in the wrapper
are based on either the Langevin equation~\cite{plangevin1908cras}%perhaps a more relevant citation required here as well?
or stochastic velocity rescaling~\cite{buss-parr08cpc}. 

The Langevin thermostat is a local thermostat, so couples to each
degree of freedom, whereas the stochastic velocity rescaling is a
global algorithm, so is only coupled to the total kinetic energy.
Global thermostats are less disruptive to the dynamics, so are much
better for calculation of properties that require reorganisation of
the structure, such as dielectric constants~\cite{ceri+10jcp}. However,
they tend be less efficient for local properties such as the total
energy.

The path integral Langevin equation (PILE) thermostat~\cite{ceri+10jcp}
adapts these algorithms for PIMD simulations, and may be either local
or global. It uses \emph{a priori} estimates for the friction coefficients
of the thermostats coupled to the internal ring polymer normal modes,
so this is much more effective for path integral calculations than
standard stochastic thermostats.

Constant temperature simulations can be run by setting the {}``type''
attribute of the {}``ensemble'' tag to {}``nvt'', and the {}``thermostat''
tag specifies how the thermostat is defined.


\subsection{Generalized Langevin equation thermostats}

The Langevin equation can be generalized to include non-Markovian
effects, so that the stochastic step of the algorithm requires information
from previous timesteps. This is however very difficult to implement
in practice.

However, this can be shown to be mathematically equivalent to a Markovian
algorithm in an extended momentum phase space~\cite{ceri+09jctc}.
This is much simpler to implement, but still maintains the increased
power and flexibility of the non-Markovian formulation. In particular,
this has two main advantages over standard Langevin equation dynamics.

Firstly, there are a large number of adjustable parameters, so the
thermostat can be fine-tuned to efficiently sample all the vibrational
frequencies present in the system rather than just a small range of
frequencies.

Secondly, the algorithm does not need to obey the fluctuation-dissipation
theorem, so the ensemble that is sampled can be adjusted manually.
For example, using information from the harmonic limit this thermostat
can attempt to enforce the appropriate quantum ensemble during a low
bead PIMD simulation~\cite{ceri+11jcp}, increasing the rate of convergence
with respect to the number of beads used.


\section{Constant pressure simulations}

Constant pressure simulations are also possible within the wrapper.
There is one barostat currently implemented~\cite{buss+09jpc}, which
is designed to be used in combination with stochastic thermostating,
so is only suitable for simulations where both the pressure and temperature
are held constant. Note that these algorithms allow the system box
size to fluctuate, which can cause problems for some driver codes
if variable cell dynamics have not been implemented.

Constant pressure simulations can be run by setting the {}``mode''
attribute of the {}``ensemble'' tag to {}``npt''. The {}``barostat''
tag specifies how the barostat is defined.


\section{Dynamical property calculations}

As well as calculating the static properties shown in section~\ref{propertyfile},
the wrapper code is capable of calculating approximations to dynamical
properties such as correlation functions. Currently, two different
techniques have been implemented for this, ring polymer molecular
dynamics (RPMD)~\cite{crai-mano04jcp} and partially adiabatic centroid
molecular dynamics (PA-CMD)~\cite{habe+08jcp}.

RPMD uses the trajectories generated during a PIMD run to calculate
correlation functions, so is straightforward to use. The largest difference
between RPMD and PIMD is that local thermostats usually disrupt the
particle motion too much to be used in the calculation of dynamical
properties, so often a constant energy ensemble must be sampled instead.
An example of one way to run an RPMD simulation is given in one of
the test directories, as explained in section~\ref{tests}.

PA-CMD uses different dynamics to calculate correlation functions.
In particular, the ring polymer bead masses are scaled, shifting the
ring polymer normal mode frequencies. This is particularly useful
where non-zero frequency components of the correlation functions are
important, as the ring polymer normal mode frequencies can contaminate
the spectrum. PA-CMD avoids this by shifting these frequencies out
of the frequency range of interest.

PA-CMD dynamics can be used by adjusting the {}``mode'' attribute
of the {}``normal\_modes'' tag. {}``normal modes'' takes an array
argument, which can be used to specify how the ring polymer normal
mode frequencies are defined.


\section{Output properties}

While most of the properties that can be output during a \ipi simulation
are self-explanatory, there are some for which the algorithms used
or the properties calculated are non-trivial and so deserve some comment.


\subsection{Displaced path momentum distribution estimators}

Momentum distributions are very difficult quantities to calculate,
because they require knowledge of the off-diagonal elements of the
density matrix. In a path-integral context, this means that the distribution
of open paths, where two of the neighbouring beads in the ring polymer
are not connected by springs, must be sampled. However, since the
ensemble is defined by the distribution of closed paths, only one
path can be opened in this way at a time without introducing sampling
errors. 

Multiple techniques have been proposed to try and alleviate the problems
of poor statistical sampling that this causes, but in the wrapper
only the displaced path estimator of \cite{linlin+10prl} has been
implemented. This uses a closed path integral simulation to generate
the dynamics, and only opens the paths when the estimator is to be
calculated, so no sampling errors are introduced by calculating this
property for multiple atoms during a particular timestep.

This therefore gives better statistical sampling, as the property
can be calculated for all identical atoms in a particular timestep
without introducing errors. Also, since the vector that opens the
path can be chosen by the user, this allows the phase space to be
searched systematically. The main disadvantages of this method are
first that every time the property is calculated a new force calculation
must be done, and secondly that convergence is very poor for highly
quantum systems. The latter problem can be circumvented by a thermodynamic
integration scheme, although this reintroduces the sampling error
problem that the open path simulations have. See \cite{linlin+10prl}
for a more in depth discussion of the method.


\subsection{Finite difference energy and heat capacity estimators}

As the momenta associated with the beads in a PIMD simulation do not
correspond to physical variables, they cannot be used to help calculate
statistical averages. Momentum dependent properties are therefore
usually calculated using identities derived from thermodynamics. For
example, the kinetic energy is calculated by:

\begin{align} \langle \hat{T} \rangle = \langle \hat{H} \rangle - \langle \hat{V} \rangle = - \frac{\partial \, \textrm{ln}(Z)}{\partial \beta} - \langle \hat{V} \rangle \end{align} 

This can then be used to derive an estimator for the kinetic energy
using the ring polymer Hamiltonian to calculate \(Z\). There are
two possible ways of representing the result. If the result of the
above calculation is used directly, then an estimator that only depends
on the positions of the beads is derived, but it involves the difference
of two large terms that scale with the number of beads and so has
a variance that increases linearly with the number of beads used.
The second result uses the virial theorem to derive another estimator
with the same average value, but with better statistics. This method
however involves the derivative of the potential with respect to the
bead positions.

For the kinetic energy, we use the virial form, since we already must
have the first derivative of the potential (i.e. the forces). However,
for other quantities such as the heat capacity, the virial estimator
involves second or higher derivatives, and so is much more costly
to evaluate.

Instead of having to compromise between statistical sampling and computational
expense, we take the approach of \cite{tyamamoto05jcp} and calculate
the temperature derivative by finite differences. While this means
that each step will require two extra force calculations it has the
same statistical properties as the virial estimator without requiring
potential derivatives to be calculated, so is often the most efficient
method to calculate such properties.


\subsection{Extras list}

In many cases, the driver code calculation will derive important quantities
other than the potential, forces and virial, which may be necessary
to calculate properties of interest such as the charge distribution.
It is therefore useful to have a mechanism by which such data can
be passed back to the wrapper code, so that it does not have to be
recalculated as a post-processing step. This is done in the code using
an {}``extras'' string. This is passed at the end of the driver
message, and read directly into a string by the wrapper. This can
then be printed directly to file using the {}``extras'' trajectory
output.

\begin{comment}

\chapter{Developer's tips}

\label{dev}


\section{Directory structure}

The code is separated into the following directory structure: 
\begin{description}
\item [{forces:}] Contains a few short fortran driver files for running
simple tests. 
\item [{doc:}] Contains the documentation, including this manual. 
\item [{src:}] Contains the source code for the wrapper. This is further
subdivided into: 

\begin{description}
\item [{engine:}] Holds the modules containing the internally used objects
and algorithms 
\item [{driver:}] Holds the modules containing the objects that deal with
the driver communication and data transfer. 
\item [{utils:}] Holds the modules containing the utility functions used
in the other modules. 
\item [{inputs:}] Holds the modules containing the classes that read the
input files.
\end{description}
\item [{patches:}] Contains the patch files that must be applied to allow
the driver codes to interact with the wrapper. 
\item [{test:}] Contains examples that can be used to make sure the build
has been successful, show the correct form of the input files, and
show some of the capabilities of the wrapper. 
\end{description}

\section{Dependency detection and automatic property caching}


\subsection{Rationale}

One of the utility classes that is defined in the code, from which
most of the classes in the code inherit, is the dobject class. Similarly,
many of the variables contained in these classes are depend objects.

The depend object class has been designed so that every time it is
accessed it checks all of the other depend objects on which it is
dependent to see if they have changed, and if so recalculates its
value. The dobject class overrides the mechanism by which you can
access the members of an instance of it, so that they are accessed
via the dependency detection functions.

These classes have been defined so that, instead of cluttering up
the code with functions to recalculate the various parameters of the
simulation, we simply have to define the function by which to recalculate
them and the other objects which they depend upon when they are created,
and then they will automatically be updated when required.

As an example, take the total kinetic energy for one replica of the
system. This is a property of the total momentum vector and the mass
vector of the different atoms. Therefore, we would create a {}``kin''
object in the atoms class that has dependencies {}``mass'' and {}``momentum'',
and a recalculation function that calculated \(\frac{\bf{p}^2}{2\bf{m}}\)

Having done this, any time we access atoms.kin, the code will check
to see if either the mass vector or momentum vector has changed. If
it has will recalculate the kinetic energy before returning it, otherwise
it will simply return the cached value.


\subsection{Synchronized objects}

As well as checking for dependencies, the same infrastructure can
also be used to synchronize two objects which are different representations
of the same data. A good example of this is the bead coordinates in
the cartesian and normal mode representations. Both of these represent
the same data, but have different numerical values.

In this case we can use a synchronize object, which holds a dictionary
of functions to transform one representation to another. In the case
of the normal mode and cartesian representations of position, the
transformation can be done either by multiplying the position vector
by a transformation matrix, or via a Fourier transform.

Using this class, whenever we make a change to one of the representations,
the others are automatically recalculated by reapplying the transformation
function the next time they are accessed.


\section{Properties}

While many of the simple properties of interest can be output directly
to the properties output file, and many others can be obtained by
post-processing the available data, it may be that some property you
wish to calculate is not explicitly available for output.

In this case, you may wish to modify the code so that this property
can be output. All the code to do this is found in the {}``properties.py''
module in the {}``engine'' directory. To add a new property to the
existing list, you must first define a function that calculates this
property.

In standard Python notation, we must define a new function inside
the {}``Properties'' class. This is written by:

\begin{verbatim}
def new_func(self):
   ...code here...
   return value
\end{verbatim}

We can now use this function to calculate the property of interest.
If the name of this property is {}``new\_prop'', we can add this
to the dictionary of named properties by:

\begin{verbatim}
self.property_dict["new_prop"] = new_func
\end{verbatim}

We now simply need to add the name {}``new\_prop'' in the {}``properties''
list in the xml input file, and the property will be written to the
properties output file when appropriate.

If you need a function with arguments, then the code we need is given
by:

\begin{verbatim}
self.property_dict["new_arg_prop"] = new_arg_func
...code here...
def new_arg_func(self, arg1, arg2, ... ):
   ...more code here...
   return value
\end{verbatim}

Now in the property list you must put:

\begin{verbatim}
<properties>
   [ new_arg_prop(arg1; arg2; ... ), ... ]
</properties>
\end{verbatim}

The code will now automatically call the function with the arguments
defined in the properties list.


\section{Creating a new class}

While creating a new property may be possible by using existing objects,
it may be that you wish to calculate something in a way that is not
implemented by the code. In this case, it may be necessary to create
a new class to do the calculation. To do this efficiently, you must
be understand some of the more advanced features of the code.


\subsection{Depend objects}

Many of the new objects created inside a class may depend on other
objects in the code for their value. As discussed in Chapter 3, the
calculation of such variables is facilitated by the use of the {}``depend''
classes defined in the {}``utils'' directory.

Since the {}``dobject'' class overwrites the standard way of getting
and setting functions, so there is a special syntax for creating them.
To create a depend object {}``value'' with name {}``name'' inside
an object {}``object'', you need to use the syntax: 

\begin{verbatim}
dset(object, name, value)
\end{verbatim}

The arguments that are important for creating {}``depend'' objects
are:
\begin{description}
\item [{name:}] A string giving the name of the depend object.
\item [{value:}] {}``depend\_array'' only. Defines the memory the object
references.
\item [{synchro:}] An optional {}``synchronizer'' object. This keeps
track of different views on the same data.
\item [{func:}] An optional function to recalculate the object when one
of its dependencies are updated.
\item [{dependencies:}] An optional list of the {}``depend'' objects
which are required to recalculate the new {}``depend'' object. Essentially
a list of all the {}``depend'' objects that appear in \textbf{func}.
\end{description}
There are two different classes of {}``depend'' objects, {}``depend\_value''
and {}``depend\_array''. The {}``depend\_array'' class is a specialist
class for NumPy arrays, to deal with the slicing mechanism. This means
that for the {}``depend\_array'' class there is a second method
of creating new {}``depend'' objects, you can take a slice of an
old one. 

This will return a slice with the same dependencies and synchro objects,
and a reference to the correct slice of the base array of the parent
{}``depend\_array'' object, so that the automatic updating keeps
all the data consistent. Note that it is therefore incorrect to create
a new {}``depend\_array'' object that is a slice of an old one using
the \textbf{dset} function, as it will not copy the dependencies correctly.

There is also a method for dealing with separate objects that are
different views on the same data; the {}``synchronizer'' object.
This object is shared with all of the different views to the same
data, and deals with keeping all of them consistent with each other.

Also, the \textbf{func} initialization argument must be a dictionary
of functions that transform from the {}``depend'' object being created
to the other synchronized objects, using the syntax:

\begin{verbatim}
func = {synchro1: func1, synchro2: func2, ... }
\end{verbatim}

Where {}``synchroN'' is the {}``name'' of the Nth synchro object,
and {}``funcN'' transforms from the {}``synchroN'' object to the
one being created. Note that we have to make sure that we do not mix
the two forms of \textbf{func}, synchonized objects can only depend
on the other synchronized objects, and not on any other depend objects.

Finally, there are several utility functions that help to manipulate
{}``depend'' objects:
\begin{description}
\item [{dget:}] Gets a specified {}``depend'' object inside a {}``dobject''
class. Needed since the {}``dobject'' class overrides the standard
Python method of getting attributes, so you need a way to get the
{}``depend'' object and not just its value.
\item [{dset:}] Creates a {}``depend'' object inside a {}``dobject''
class. Needed for the same reason as \textbf{dget}.
\item [{depstrip:}] Takes a {}``depend\_array'' object and returns the
base array. Useful in algorithms where the array is used as a read-only
variable, as it removes the dependancy network from the object, and
so speeds up the calculation.
\item [{depcopy:}] Copies the {}``dependencies'' and {}``synchronizer''
objects from one {}``depend'' object to another.
\item [{deppipe:}] Gives one {}``depend'' object the same value as another,
and adds a dependency such that this remains the case.
\end{description}

\subsection{Input class}

To be able to use the xml input file to create an instance of this
new class, and to write the file to the restart file, you must create
a new input class, or modify an old one to recognize the new class.

Each input class must have the following class attributes:
\begin{description}
\item [{fields:}] This is a dictionary giving the tag names for the objects
that can be specified in the xml input file within the tags of your
class. Every object that can be given a value by the user should be
a keyword of this dictionary, or of the \textbf{attributes }dictionary.
Within the dictionary, you must specify the particular class of input
object that will be created and arguments to initialize it. The format
of the dictionary is: \begin{verbatim}
{tag_name1 :
   (InputClass, 
      {default: default value,
       help: help string, 
       dtype: data type,
       options: [option1, option2, ... ],
       dimension: dimensionality of the data}), 
tag_name2: ... }
\end{verbatim}
\item [{attribs:}] This is a dictionary of the same form as \textbf{fields},
but for the tag attribute data instead. These should always be simple
data classes InputValue or InputArray, as their value must be specified
by the user.
\item [{default\_help:}] This is the help string that will be given to
help explain the new class to others if no other help string is given.
It will also be the help string seen in the section in the latex file
for the new class.
\item [{default\_label:}] This is the label used to identify the appropriate
section in the latex help file so that different sections can cross-reference
with the others.
\end{description}
The class can have the following functions:
\begin{description}
\item [{fetch:}] This takes the InputClass object and creates a new object
of your class from it. You must make sure to call the base class fetch
function using the standard Python function super.
\item [{store:}] This takes an object of your class, and stores the relevant
information needed to reinitialize it in an InputClass object. You
must make sure to call the base class store function using the standard
Python function super.
\item [{check:}] This is an optional function that will check to see if
the data specified in the input file will create a valid instance
of your class, and possibly raise an exception otherwise. You must
make sure to call the base class check function using the standard
Python function super.
\item [{write:}] This is a function that you can use to override the base
class write function, so that you can tailor what is printed to the
restart file.
\end{description}
The last thing that you must do to be able to use your class is to
create an appropriate fields tag in the parent input class where you
want the instance of your class to be held, with InputClass set to
the name of your input class, and alter the check, store and fetch
functions of the input class of the parent class.


\section{Submitting improvements}

This code is still in developement, and any submissions to improve
the code are welcome. However, to be considered as a candidate for
inclusion in the main version of the code additions must adhere to
the standards described in this section.


\subsection{Testing}

Any new code should be tested for bugs before being submitted. At
the very least the modified code should reproduce the properties in
the test directory, and be shown to give the expected new results.
Preferably, new results found with wrap-PI should also be compared
with other published results, and a suitable test case added to the
test directory.

It should also be tested for user input error, especially if a new
input class has been created, so that such errors can be detected
before the simulation is run. For example, if a parameter cannot be
negative then this should be tested in the code, so that a sensible
error message is given if the user inputs a negative value.


\subsection{Documentation}

We have taken great care to make sure that all the code is well documented,
and as such any submissions should be documented to the same standard.
We have used the Google Python Style Guide, \url{http://google-styleguide.googlecode.com/svn/trunk/pyguide.html},
as a template for the documentation.

For each module we require the following documentation:
\begin{itemize}
\item Each new module must be added to the \_\_init\_\_.py and README files
of the directory in which it is found.
\item At the head of each new module, you must give a string with a summary
and a brief description of the module with all the classes and functions
that it defines, and an \_\_all\_\_ object giving the same classes
and functions as above.
\item Each class must have a docstring giving a summary, a brief description
of its purpose and the variables it contains, including a separate
section for any depend objects defined, and what their dependencies
are. Input classes must also have a help string defined for all its
tags, and an appropriate default tag.
\item Each function must have a docstring giving a summary, a brief description
of its purpose, any arguments it uses, any exceptions it can raise
and what is returned by the function, if not None. 
\item Any new class must have its input class added to the create\_man.py
and help.py files in the doc directory, so that the automatic manual
creation will work correctly.
\end{itemize}
If new modules are being submitted, then all of the above documentation
should be done. If only a new class or function is being added, then
only parts of the above will be necessary.
\end{comment}



\chapter{Input class hierarchy}

%\label{hierarchy}

The following chapter gives a complete list of the tags that can be
specified in the xml input file, along with the hierarchy of objects.
Note that every xml input file must start with the root tag {}``simulation''. 

See the accompanying {}``help.xml'' file in the {}``doc'' directory
to see the recommended input file structure.
\end{document}
